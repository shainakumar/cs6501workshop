{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ae2d9",
   "metadata": {},
   "source": [
    "# Create a Python environment with the following modules installed by conda or pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf097ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch datasets accelerate tqdm huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33142549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d28f2",
   "metadata": {},
   "source": [
    "# Set up your computing environment with Hugging Face authorization for Llama 3.2-1B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caeb0049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43635da0e40f460890cf296b13871035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1muser: \u001b[0m shainakumar\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login  \n",
    "notebook_login()\n",
    "!hf auth whoami "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae11fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28762625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-15.6-arm64-arm-64bit\n",
      "torch 2.5.1\n",
      "cuda False\n",
      "mps True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import platform, torch; print(platform.platform()); print('torch', torch.__version__); print('cuda', torch.cuda.is_available()); print('mps', hasattr(torch.backends,'mps') and torch.backends.mps.is_available())\" \\\n",
    "| tee outputs/env_info.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004e9b4",
   "metadata": {},
   "source": [
    "# Verify that your setup is working by running llama_mmlu_eval.py which runs the model on two MMLU topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b617cb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:42<00:00,  1.48it/s]\n",
      "âœ“ Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:58<00:00,  1.72it/s]\n",
      "âœ“ Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.7 minutes\n",
      "======================================================================\n",
      "\n",
      "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "\n",
      "ğŸ“Š Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "ğŸ“‰ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "âœ… Evaluation complete!\n",
      "real 192.80\n",
      "user 81.59\n",
      "sys 40.90\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/baseline_mps_noquant.txt /usr/bin/time -p python llama_mmlu_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1deb78",
   "metadata": {},
   "source": [
    "# Time the code using the time shell command line function. \n",
    "\n",
    "Compare the timings for the following setups:\n",
    "\n",
    "Using GPU and no quantization.\n",
    "\n",
    "Using GPU and 4-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using GPU and 8-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using CPU and no quantization.\n",
    "\n",
    "Using CPU and 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649c5f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic1LLMs.ipynb\n",
      "eval_cpu_noquant.py\n",
      "eval_mps_noquant.py\n",
      "llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "llama_mmlu_eval.py\n",
      "\u001b[34moutputs\u001b[m\u001b[m\n",
      "\u001b[34mplots\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!cp llama_mmlu_eval.py eval_mps_noquant.py\n",
    "!cp llama_mmlu_eval.py eval_cpu_noquant.py\n",
    "!mkdir -p outputs\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2875102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:29<00:00,  1.70it/s]\n",
      "âœ“ Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:48<00:00,  2.04it/s]\n",
      "âœ“ Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.3 minutes\n",
      "======================================================================\n",
      "\n",
      "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "\n",
      "ğŸ“Š Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "ğŸ“‰ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "âœ… Evaluation complete!\n",
      "real 170.45\n",
      "user 72.62\n",
      "sys 33.63\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_mps_noquant.txt /usr/bin/time -p python eval_mps_noquant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360b7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mTopic1LLMs.ipynb\u001b[m\n",
      "\t\u001b[31meval_cpu_noquant.py\u001b[m\n",
      "\t\u001b[31meval_mps_noquant.py\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_212449.json\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_213254.json\u001b[m\n",
      "\t\u001b[31mllama_mmlu_eval.py\u001b[m\n",
      "\t\u001b[31moutputs/baseline_mps_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/env_info.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_cpu_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_mps_noquant.txt\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status --untracked-files=all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5541a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: pathspec 'Running_an_LLM' did not match any files\n",
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31m./\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe6163e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ          | 83/152 [01:39<01:08,  1.01it/s]"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 5] Input/output error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/IPython/utils/_process_posix.py:130\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    128\u001b[39m     \u001b[38;5;66;03m# res is the index of the pattern that caused the match, so we\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;66;03m# know whether we've finished (if we matched EOF) or not\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m     res_idx = \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpatterns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    131\u001b[39m     \u001b[38;5;28mprint\u001b[39m(child.before[out_size:].decode(enc, \u001b[33m'\u001b[39m\u001b[33mreplace\u001b[39m\u001b[33m'\u001b[39m), end=\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/spawnbase.py:383\u001b[39m, in \u001b[36mSpawnBase.expect_list\u001b[39m\u001b[34m(self, pattern_list, timeout, searchwindowsize, async_, **kw)\u001b[39m\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m383\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mexp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpect_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/expect.py:169\u001b[39m, in \u001b[36mExpecter.expect_loop\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Still have time left, so read more data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m incoming = \u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_nonblocking\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaxread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.spawn.delayafterread \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/pty_spawn.py:500\u001b[39m, in \u001b[36mspawn.read_nonblocking\u001b[39m\u001b[34m(self, size, timeout)\u001b[39m\n\u001b[32m    497\u001b[39m \u001b[38;5;66;03m# Because of the select(0) check above, we know that no data\u001b[39;00m\n\u001b[32m    498\u001b[39m \u001b[38;5;66;03m# is available right now. But if a non-zero timeout is given\u001b[39;00m\n\u001b[32m    499\u001b[39m \u001b[38;5;66;03m# (possibly timeout=None), we call select() with a timeout.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (timeout != \u001b[32m0\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(spawn, \u001b[38;5;28mself\u001b[39m).read_nonblocking(size)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/pty_spawn.py:450\u001b[39m, in \u001b[36mspawn.read_nonblocking.<locals>.select\u001b[39m\u001b[34m(timeout)\u001b[39m\n\u001b[32m    449\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mselect\u001b[39m(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect_ignore_interrupts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_fd\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/utils.py:143\u001b[39m, in \u001b[36mselect_ignore_interrupts\u001b[39m\u001b[34m(iwtd, owtd, ewtd, timeout)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mselect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43miwtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mowtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mewtd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mscript -q outputs/timing_cpu_noquant.txt /usr/bin/time -p python eval_cpu_noquant.py\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/ipykernel/zmqshell.py:788\u001b[39m, in \u001b[36mZMQInteractiveShell.system_piped\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    786\u001b[39m         \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = system(cmd)\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     \u001b[38;5;28mself\u001b[39m.user_ns[\u001b[33m\"\u001b[39m\u001b[33m_exit_code\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[43msystem\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvar_expand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/IPython/utils/_process_posix.py:141\u001b[39m, in \u001b[36mProcessHandler.system\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    136\u001b[39m         out_size = \u001b[38;5;28mlen\u001b[39m(child.before)\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[32m    138\u001b[39m     \u001b[38;5;66;03m# We need to send ^C to the process.  The ascii code for '^C' is 3\u001b[39;00m\n\u001b[32m    139\u001b[39m     \u001b[38;5;66;03m# (the character is known as ETX for 'End of Text', see\u001b[39;00m\n\u001b[32m    140\u001b[39m     \u001b[38;5;66;03m# curses.ascii.ETX).\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m141\u001b[39m     \u001b[43mchild\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mchr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m     \u001b[38;5;66;03m# Read and print any more output the program might produce on its\u001b[39;00m\n\u001b[32m    143\u001b[39m     \u001b[38;5;66;03m# way out.\u001b[39;00m\n\u001b[32m    144\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/pty_spawn.py:578\u001b[39m, in \u001b[36mspawn.sendline\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    572\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Wraps send(), sending string ``s`` to child process, with\u001b[39;00m\n\u001b[32m    573\u001b[39m \u001b[33;03m``os.linesep`` automatically appended. Returns number of bytes\u001b[39;00m\n\u001b[32m    574\u001b[39m \u001b[33;03mwritten.  Only a limited number of bytes may be sent for each\u001b[39;00m\n\u001b[32m    575\u001b[39m \u001b[33;03mline in the default terminal mode, see docstring of :meth:`send`.\u001b[39;00m\n\u001b[32m    576\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    577\u001b[39m s = \u001b[38;5;28mself\u001b[39m._coerce_send_string(s)\n\u001b[32m--> \u001b[39m\u001b[32m578\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinesep\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/myenv/lib/python3.12/site-packages/pexpect/pty_spawn.py:569\u001b[39m, in \u001b[36mspawn.send\u001b[39m\u001b[34m(self, s)\u001b[39m\n\u001b[32m    566\u001b[39m \u001b[38;5;28mself\u001b[39m._log(s, \u001b[33m'\u001b[39m\u001b[33msend\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    568\u001b[39m b = \u001b[38;5;28mself\u001b[39m._encoder.encode(s, final=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m569\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_fd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mOSError\u001b[39m: [Errno 5] Input/output error"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_cpu_noquant.txt /usr/bin/time -p python eval_cpu_noquant.py\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
