{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ae2d9",
   "metadata": {},
   "source": [
    "# Create a Python environment with the following modules installed by conda or pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf097ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch datasets accelerate tqdm huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33142549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (8.1.8)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.14 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (4.0.15)\n",
      "Requirement already satisfied: jupyterlab_widgets~=3.0.15 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (3.0.16)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d28f2",
   "metadata": {},
   "source": [
    "# Set up your computing environment with Hugging Face authorization for Llama 3.2-1B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "caeb0049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4145e913904489803e7b43088e5ad5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1muser: \u001b[0m shainakumar\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login  \n",
    "notebook_login()\n",
    "!hf auth whoami "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae11fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28762625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-15.6-arm64-arm-64bit\n",
      "torch 2.5.1\n",
      "cuda False\n",
      "mps True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import platform, torch; print(platform.platform()); print('torch', torch.__version__); print('cuda', torch.cuda.is_available()); print('mps', hasattr(torch.backends,'mps') and torch.backends.mps.is_available())\" \\\n",
    "| tee outputs/env_info.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004e9b4",
   "metadata": {},
   "source": [
    "# Verify that your setup is working by running llama_mmlu_eval.py which runs the model on two MMLU topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b617cb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "‚úì Running locally (not in Colab)\n",
      "‚úì Platform: Darwin (arm64)\n",
      "‚úì Processor: arm\n",
      "‚úì Apple Metal (MPS) Available\n",
      "‚úì Using Metal Performance Shaders for GPU acceleration\n",
      "‚úì Quantization disabled - loading full precision model\n",
      "‚úì Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "‚úì Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [01:42<00:00,  1.48it/s]\n",
      "‚úì Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:58<00:00,  1.72it/s]\n",
      "‚úì Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.7 minutes\n",
      "======================================================================\n",
      "\n",
      "‚úì Results saved to: llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "\n",
      "üìä Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "üìâ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "real 192.80\n",
      "user 81.59\n",
      "sys 40.90\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/baseline_mps_noquant.txt /usr/bin/time -p python llama_mmlu_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1deb78",
   "metadata": {},
   "source": [
    "# Time the code using the time shell command line function. \n",
    "\n",
    "Compare the timings for the following setups:\n",
    "\n",
    "Using GPU and no quantization.\n",
    "\n",
    "Using GPU and 4-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using GPU and 8-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using CPU and no quantization.\n",
    "\n",
    "Using CPU and 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649c5f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic1LLMs.ipynb\n",
      "eval_cpu_noquant.py\n",
      "eval_mps_noquant.py\n",
      "llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "llama_mmlu_eval.py\n",
      "\u001b[34moutputs\u001b[m\u001b[m\n",
      "\u001b[34mplots\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!cp llama_mmlu_eval.py eval_mps_noquant.py\n",
    "!cp llama_mmlu_eval.py eval_cpu_noquant.py\n",
    "!mkdir -p outputs\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2875102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "‚úì Running locally (not in Colab)\n",
      "‚úì Platform: Darwin (arm64)\n",
      "‚úì Processor: arm\n",
      "‚úì Apple Metal (MPS) Available\n",
      "‚úì Using Metal Performance Shaders for GPU acceleration\n",
      "‚úì Quantization disabled - loading full precision model\n",
      "‚úì Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "‚úì Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [01:29<00:00,  1.70it/s]\n",
      "‚úì Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.04it/s]\n",
      "‚úì Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.3 minutes\n",
      "======================================================================\n",
      "\n",
      "‚úì Results saved to: llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "\n",
      "üìä Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "üìâ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "real 170.45\n",
      "user 72.62\n",
      "sys 33.63\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_mps_noquant.txt /usr/bin/time -p python eval_mps_noquant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360b7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mTopic1LLMs.ipynb\u001b[m\n",
      "\t\u001b[31meval_cpu_noquant.py\u001b[m\n",
      "\t\u001b[31meval_mps_noquant.py\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_212449.json\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_213254.json\u001b[m\n",
      "\t\u001b[31mllama_mmlu_eval.py\u001b[m\n",
      "\t\u001b[31moutputs/baseline_mps_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/env_info.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_cpu_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_mps_noquant.txt\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status --untracked-files=all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5541a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mnew file:   Topic1LLMs.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   eval_cpu_noquant.py\u001b[m\n",
      "\t\u001b[32mnew file:   eval_mps_noquant.py\u001b[m\n",
      "\t\u001b[32mnew file:   llama_3.2_1b_mmlu_results_full_20260131_212449.json\u001b[m\n",
      "\t\u001b[32mnew file:   llama_3.2_1b_mmlu_results_full_20260131_213254.json\u001b[m\n",
      "\t\u001b[32mnew file:   llama_mmlu_eval.py\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/baseline_mps_noquant.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/env_info.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/timing_cpu_noquant.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/timing_mps_noquant.txt\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa5524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main ecd4657] baseline timings and setup\n",
      " 10 files changed, 2415 insertions(+)\n",
      " create mode 100644 Running_an_LLM/Topic1LLMs.ipynb\n",
      " create mode 100644 Running_an_LLM/eval_cpu_noquant.py\n",
      " create mode 100644 Running_an_LLM/eval_mps_noquant.py\n",
      " create mode 100644 Running_an_LLM/llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      " create mode 100644 Running_an_LLM/llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      " create mode 100644 Running_an_LLM/llama_mmlu_eval.py\n",
      " create mode 100644 Running_an_LLM/outputs/baseline_mps_noquant.txt\n",
      " create mode 100644 Running_an_LLM/outputs/env_info.txt\n",
      " create mode 100644 Running_an_LLM/outputs/timing_cpu_noquant.txt\n",
      " create mode 100644 Running_an_LLM/outputs/timing_mps_noquant.txt\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"baseline timings and setup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fd9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 14, done.\n",
      "Counting objects: 100% (14/14), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (13/13), done.\n",
      "Writing objects: 100% (13/13), 22.30 KiB | 7.43 MiB/s, done.\n",
      "Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), done.\u001b[K\n",
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      "   12116bb..ecd4657  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6163e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17766) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "‚úì Running locally (not in Colab)\n",
      "‚úì Platform: Darwin (arm64)\n",
      "‚úì Processor: arm\n",
      "‚ö†Ô∏è  No GPU detected - running on CPU\n",
      "‚úì Quantization disabled - loading full precision model\n",
      "‚úì Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: cpu\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~5 GB (FP32)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: cpu\n",
      "‚úì Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: cpu\n",
      "  Model dtype: torch.float32\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [10:23<00:00,  4.10s/it]\n",
      "‚úì Result: 75/152 correct = 49.34%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [04:03<00:00,  2.43s/it]\n",
      "‚úì Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 120\n",
      "Overall Accuracy: 47.62%\n",
      "Duration: 14.5 minutes\n",
      "======================================================================\n",
      "\n",
      "‚úì Results saved to: llama_3.2_1b_mmlu_results_full_20260201_092415.json\n",
      "\n",
      "üìä Top 5 Subjects:\n",
      "  1. astronomy: 49.34%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "üìâ Bottom 5 Subjects:\n",
      "  1. astronomy: 49.34%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "real 890.78\n",
      "user 1888.90\n",
      "sys 474.24\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_cpu_noquant.txt /usr/bin/time -p python eval_cpu_noquant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76136178",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add .\n",
    "!git commit -m \"local cpu eval\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7910a",
   "metadata": {},
   "source": [
    "# Modify the code to do the following:\n",
    "\n",
    "Run on a selection of 10 subjects using 2 other tiny/small models in addtion to Llama 3.2-1B.  \n",
    "\n",
    "Add timing information to the evaluation summary showing the cycles consumed by each model.  Include all of real time, CPU time, and GPU time.\n",
    "\n",
    "Add an option to the program to make it print out each question, the answer the model gives, and whether the answer is right or wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d37050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp llama_mmlu_eval.py modified_mmlu_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79aa314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Multi-Model MMLU Evaluation\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "‚úì Running locally (not in Colab)\n",
      "‚úì Platform: Darwin (arm64)\n",
      "‚úì Processor: arm\n",
      "‚úì Apple Metal (MPS) Available\n",
      "‚úì Using Metal Performance Shaders for GPU acceleration\n",
      "‚úì Quantization disabled - loading full precision model\n",
      "‚úì Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Models: 3\n",
      "  - meta-llama/Llama-3.2-1B-Instruct\n",
      "  - TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  - Qwen/Qwen2.5-0.5B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory per model: ~2.5 GB (FP16)\n",
      "Number of subjects: 10\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:18<00:00,  1.28it/s]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚úì Timing: Real: 77.60s, CPU: 51.25s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [01:36<00:00,  1.40it/s]\n",
      "\n",
      "‚úì Result: 65/135 correct = 48.15%\n",
      "‚úì Timing: Real: 95.47s, CPU: 64.46s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [01:16<00:00,  2.00it/s]\n",
      "\n",
      "‚úì Result: 76/152 correct = 50.00%\n",
      "‚úì Timing: Real: 75.91s, CPU: 49.60s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:46<00:00,  2.15it/s]\n",
      "\n",
      "‚úì Result: 45/100 correct = 45.00%\n",
      "‚úì Timing: Real: 46.38s, CPU: 30.11s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [01:39<00:00,  2.67it/s]\n",
      "\n",
      "‚úì Result: 144/265 correct = 54.34%\n",
      "‚úì Timing: Real: 98.85s, CPU: 65.72s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [01:12<00:00,  1.99it/s]\n",
      "\n",
      "‚úì Result: 76/144 correct = 52.78%\n",
      "‚úì Timing: Real: 72.11s, CPU: 44.28s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:09<00:00,  1.43it/s]\n",
      "\n",
      "‚úì Result: 33/100 correct = 33.00%\n",
      "‚úì Timing: Real: 69.49s, CPU: 47.65s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:29<00:00,  1.12it/s]\n",
      "\n",
      "‚úì Result: 25/100 correct = 25.00%\n",
      "‚úì Timing: Real: 89.39s, CPU: 55.81s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:55<00:00,  1.81it/s]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚úì Timing: Real: 54.99s, CPU: 30.68s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:50<00:00,  1.56it/s]\n",
      "\n",
      "‚úì Result: 79/173 correct = 45.66%\n",
      "‚úì Timing: Real: 110.44s, CPU: 60.28s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "tokenizer_config.json: 1.29kB [00:00, 418kB/s]\n",
      "tokenizer.model: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 500k/500k [00:01<00:00, 415kB/s]\n",
      "tokenizer.json: 1.84MB [00:00, 19.6MB/s]\n",
      "special_tokens_map.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 551/551 [00:00<00:00, 2.38MB/s]\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 608/608 [00:00<00:00, 6.34MB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2.20G/2.20G [01:41<00:00, 21.6MB/s]\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 124/124 [00:00<00:00, 1.15MB/s]\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [08:31<00:00,  5.12s/it]\n",
      "\n",
      "‚úì Result: 15/100 correct = 15.00%\n",
      "‚úì Timing: Real: 509.39s, CPU: 237.01s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [05:10<00:00,  2.30s/it]\n",
      "\n",
      "‚úì Result: 32/135 correct = 23.70%\n",
      "‚úì Timing: Real: 308.88s, CPU: 163.33s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [03:09<00:00,  1.24s/it]\n",
      "\n",
      "‚úì Result: 34/152 correct = 22.37%\n",
      "‚úì Timing: Real: 187.99s, CPU: 115.56s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [03:09<00:00,  1.89s/it]\n",
      "\n",
      "‚úì Result: 22/100 correct = 22.00%\n",
      "‚ö†Ô∏è  Invalid responses: 1/100 (1.0%)\n",
      "‚úì Accuracy on valid responses: 22/99 = 22.22%\n",
      "‚úì Timing: Real: 187.98s, CPU: 85.45s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [07:20<00:00,  1.66s/it]\n",
      "\n",
      "‚úì Result: 75/265 correct = 28.30%\n",
      "‚úì Timing: Real: 437.11s, CPU: 184.44s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [04:57<00:00,  2.07s/it]\n",
      "\n",
      "‚úì Result: 29/144 correct = 20.14%\n",
      "‚úì Timing: Real: 296.03s, CPU: 149.91s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [04:09<00:00,  2.50s/it]\n",
      "\n",
      "‚úì Result: 32/100 correct = 32.00%\n",
      "‚ö†Ô∏è  Invalid responses: 2/100 (2.0%)\n",
      "‚úì Accuracy on valid responses: 32/98 = 32.65%\n",
      "‚úì Timing: Real: 248.50s, CPU: 105.19s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [05:59<00:00,  3.60s/it]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚ö†Ô∏è  Invalid responses: 4/100 (4.0%)\n",
      "‚úì Accuracy on valid responses: 24/96 = 25.00%\n",
      "‚úì Timing: Real: 358.56s, CPU: 168.21s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:21<00:00,  1.41s/it]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 5/100 (5.0%)\n",
      "‚úì Accuracy on valid responses: 29/95 = 30.53%\n",
      "‚úì Timing: Real: 140.63s, CPU: 66.05s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [12:53<00:00,  4.47s/it]\n",
      "\n",
      "‚úì Result: 37/173 correct = 21.39%\n",
      "‚ö†Ô∏è  Invalid responses: 1/173 (0.6%)\n",
      "‚úì Accuracy on valid responses: 37/172 = 21.51%\n",
      "‚úì Timing: Real: 766.90s, CPU: 295.09s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "tokenizer_config.json: 7.30kB [00:00, 520kB/s]\n",
      "vocab.json: 2.78MB [00:00, 13.0MB/s]\n",
      "merges.txt: 1.67MB [00:00, 13.4MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 20.4MB/s]\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 659/659 [00:00<00:00, 1.72MB/s]\n",
      "model.safetensors: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 988M/988M [00:44<00:00, 22.2MB/s]\n",
      "generation_config.json: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 242/242 [00:00<00:00, 2.26MB/s]\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [21:34<00:00, 12.95s/it]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 1/100 (1.0%)\n",
      "‚úì Accuracy on valid responses: 29/99 = 29.29%\n",
      "‚úì Timing: Real: 1289.31s, CPU: 551.10s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [25:34<00:00, 11.36s/it]\n",
      "\n",
      "‚úì Result: 59/135 correct = 43.70%\n",
      "‚úì Timing: Real: 1526.67s, CPU: 548.35s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [18:41<00:00,  7.38s/it]\n",
      "\n",
      "‚úì Result: 66/152 correct = 43.42%\n",
      "‚úì Timing: Real: 1116.14s, CPU: 427.72s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:47<00:00,  5.87s/it]\n",
      "\n",
      "‚úì Result: 51/100 correct = 51.00%\n",
      "‚úì Timing: Real: 583.99s, CPU: 229.51s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [16:47<00:00,  3.80s/it]\n",
      "\n",
      "‚úì Result: 123/265 correct = 46.42%\n",
      "‚úì Timing: Real: 999.46s, CPU: 407.58s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [14:42<00:00,  6.13s/it]\n",
      "\n",
      "‚úì Result: 62/144 correct = 43.06%\n",
      "‚ö†Ô∏è  Invalid responses: 1/144 (0.7%)\n",
      "‚úì Accuracy on valid responses: 62/143 = 43.36%\n",
      "‚úì Timing: Real: 878.63s, CPU: 345.76s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:50<00:00,  5.90s/it]\n",
      "\n",
      "‚úì Result: 28/100 correct = 28.00%\n",
      "‚úì Timing: Real: 587.23s, CPU: 240.45s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [16:40<00:00, 10.01s/it]\n",
      "\n",
      "‚úì Result: 30/100 correct = 30.00%\n",
      "‚ö†Ô∏è  Invalid responses: 2/100 (2.0%)\n",
      "‚úì Accuracy on valid responses: 30/98 = 30.61%\n",
      "‚úì Timing: Real: 996.22s, CPU: 365.57s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [06:56<00:00,  4.17s/it]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 12/100 (12.0%)\n",
      "‚úì Accuracy on valid responses: 29/88 = 32.95%\n",
      "‚úì Timing: Real: 414.97s, CPU: 166.37s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [22:56<00:00,  7.96s/it]\n",
      "\n",
      "‚úì Result: 71/173 correct = 41.04%\n",
      "‚úì Timing: Real: 1370.58s, CPU: 505.21s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "MULTI-MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Device: mps\n",
      "None (full precision)\n",
      "Total Subjects: 10\n",
      "Total Duration: 240.4 minutes\n",
      "======================================================================\n",
      "\n",
      "meta-llama/Llama-3.2-1B-Instruct:\n",
      "  Overall Accuracy: 43.17% (591/1369)\n",
      "  Timing:\n",
      "    Real Time: 790.64s (13.2 min)\n",
      "    CPU Time:  499.85s (8.3 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0:\n",
      "  Overall Accuracy: 24.03% (329/1369)\n",
      "  Invalid Responses: 13/1369 (0.9%)\n",
      "  Accuracy on Valid: 24.26% (329/1356)\n",
      "  Timing:\n",
      "    Real Time: 3441.96s (57.4 min)\n",
      "    CPU Time:  1570.24s (26.2 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "Qwen/Qwen2.5-0.5B-Instruct:\n",
      "  Overall Accuracy: 40.03% (548/1369)\n",
      "  Invalid Responses: 16/1369 (1.2%)\n",
      "  Accuracy on Valid: 40.50% (548/1353)\n",
      "  Timing:\n",
      "    Real Time: 9763.19s (162.7 min)\n",
      "    CPU Time:  3787.63s (63.1 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "======================================================================\n",
      "\n",
      "‚úì Results saved to: multi_model_mmlu_results_full_20260201_040616.json\n",
      "\n",
      "üìä Top 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. clinical_knowledge: 54.34%\n",
      "  2. college_biology: 52.78%\n",
      "  3. astronomy: 50.00%\n",
      "\n",
      "üìâ Bottom 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. college_computer_science: 25.00%\n",
      "  2. abstract_algebra: 24.00%\n",
      "  3. college_mathematics: 24.00%\n",
      "\n",
      "üìä Top 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_chemistry: 32.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. clinical_knowledge: 28.30%\n",
      "\n",
      "üìâ Bottom 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_medicine: 21.39%\n",
      "  2. college_biology: 20.14%\n",
      "  3. abstract_algebra: 15.00%\n",
      "\n",
      "üìä Top 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. business_ethics: 51.00%\n",
      "  2. clinical_knowledge: 46.42%\n",
      "  3. anatomy: 43.70%\n",
      "\n",
      "üìâ Bottom 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. abstract_algebra: 29.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. college_chemistry: 28.00%\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "real 14488.82\n",
      "user 1267.85\n",
      "sys 4697.66\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_modified_mmlu_eval.txt /usr/bin/time -p python modified_mmlu_eval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3aa959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17470) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17475) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 29f83be] local multi model eval\n",
      " 5 files changed, 2200 insertions(+), 53 deletions(-)\n",
      " create mode 100644 Running_an_LLM/modified_mmlu_eval.py\n",
      " create mode 100644 Running_an_LLM/multi_model_mmlu_results_full_20260201_040616.json\n",
      " create mode 100644 Running_an_LLM/outputs/timing_modified_mmlu_eval.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17477) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/shainakumar/cs6501workshop.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git add . \n",
    "!git commit -m \"local multi model eval\"\n",
    "!git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf970112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17541) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   Topic1LLMs.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17542) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: cannot pull with rebase: You have unstaged changes.\n",
      "error: please commit or stash them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17547) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/shainakumar/cs6501workshop.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git status\n",
    "!git pull --rebase origin main\n",
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08f75ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17622) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved working directory and index state On main: WIP Topic1LLMs notebook\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17627) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 38, done.\u001b[K\n",
      "remote: Counting objects: 100% (38/38), done.\u001b[K\n",
      "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
      "remote: Total 36 (delta 16), reused 30 (delta 10), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (36/36), 2.15 MiB | 11.79 MiB/s, done.\n",
      "From https://github.com/shainakumar/cs6501workshop\n",
      " * branch            main       -> FETCH_HEAD\n",
      "   ecd4657..d2bb1ed  main       -> origin/main\n",
      "\u001b[KSuccessfully rebased and updated refs/heads/main.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17642) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 14, done.\n",
      "Counting objects: 100% (14/14), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (9/9), done.\n",
      "Writing objects: 100% (9/9), 80.21 KiB | 5.35 MiB/s, done.\n",
      "Total 9 (delta 5), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (5/5), completed with 5 local objects.\u001b[K\n",
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      "   d2bb1ed..94a15d0  main -> main\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17666) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   Topic1LLMs.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n",
      "Dropped refs/stash@{0} (511fb492a3dfe4e7e2279eed88c6ee27482f1b22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17668) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17669) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main b5aa99e] Update Topic1LLMs notebook\n",
      " 1 file changed, 140 insertions(+), 2 deletions(-)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17671) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 7, done.\n",
      "Counting objects: 100% (7/7), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (4/4), done.\n",
      "Writing objects: 100% (4/4), 1.38 KiB | 1.38 MiB/s, done.\n",
      "Total 4 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), completed with 3 local objects.\u001b[K\n",
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      "   94a15d0..b5aa99e  main -> main\n"
     ]
    }
   ],
   "source": [
    "# 1) stash the notebook change \n",
    "!git stash push -m \"WIP Topic1LLMs notebook\" -- Topic1LLMs.ipynb\n",
    "\n",
    "# 2) integrate remote changes cleanly\n",
    "!git pull --rebase origin main\n",
    "\n",
    "# 3) push your existing commit(s)\n",
    "!git push origin main\n",
    "\n",
    "# 4) restore your notebook change\n",
    "!git stash pop\n",
    "\n",
    "# 5) commit the notebook change and push it too\n",
    "!git add Topic1LLMs.ipynb\n",
    "!git commit -m \"Update Topic1LLMs notebook\"\n",
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38736406",
   "metadata": {},
   "source": [
    "# Rerun Local Multi Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33ec958d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Multi-Model MMLU Evaluation\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "‚úì Running locally (not in Colab)\n",
      "‚úì Platform: Darwin (arm64)\n",
      "‚úì Processor: arm\n",
      "‚úì Apple Metal (MPS) Available\n",
      "‚úì Using Metal Performance Shaders for GPU acceleration\n",
      "‚úì Quantization disabled - loading full precision model\n",
      "‚úì Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Models: 3\n",
      "  - meta-llama/Llama-3.2-1B-Instruct\n",
      "  - TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  - Qwen/Qwen2.5-0.5B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory per model: ~2.5 GB (FP16)\n",
      "Number of subjects: 10\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:48<00:00,  2.05it/s]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚úì Timing: Real: 48.61s, CPU: 37.47s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [01:10<00:00,  1.92it/s]\n",
      "\n",
      "‚úì Result: 65/135 correct = 48.15%\n",
      "‚úì Timing: Real: 70.10s, CPU: 49.42s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [00:53<00:00,  2.86it/s]\n",
      "\n",
      "‚úì Result: 76/152 correct = 50.00%\n",
      "‚úì Timing: Real: 52.95s, CPU: 34.26s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:30<00:00,  3.33it/s]\n",
      "\n",
      "‚úì Result: 45/100 correct = 45.00%\n",
      "‚úì Timing: Real: 29.95s, CPU: 18.62s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [01:04<00:00,  4.13it/s]\n",
      "\n",
      "‚úì Result: 144/265 correct = 54.34%\n",
      "‚úì Timing: Real: 64.09s, CPU: 41.64s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [00:51<00:00,  2.79it/s]\n",
      "\n",
      "‚úì Result: 76/144 correct = 52.78%\n",
      "‚úì Timing: Real: 51.41s, CPU: 33.42s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:34<00:00,  2.92it/s]\n",
      "\n",
      "‚úì Result: 33/100 correct = 33.00%\n",
      "‚úì Timing: Real: 34.19s, CPU: 22.25s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:49<00:00,  2.02it/s]\n",
      "\n",
      "‚úì Result: 25/100 correct = 25.00%\n",
      "‚úì Timing: Real: 49.37s, CPU: 34.82s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:32<00:00,  3.09it/s]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚úì Timing: Real: 32.28s, CPU: 20.29s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [01:21<00:00,  2.11it/s]\n",
      "\n",
      "‚úì Result: 79/173 correct = 45.66%\n",
      "‚úì Timing: Real: 81.79s, CPU: 51.55s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:49<00:00,  1.10s/it]\n",
      "\n",
      "‚úì Result: 15/100 correct = 15.00%\n",
      "‚úì Timing: Real: 109.55s, CPU: 79.77s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [01:14<00:00,  1.81it/s]\n",
      "\n",
      "‚úì Result: 32/135 correct = 23.70%\n",
      "‚úì Timing: Real: 74.61s, CPU: 59.42s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [01:24<00:00,  1.79it/s]\n",
      "\n",
      "‚úì Result: 34/152 correct = 22.37%\n",
      "‚úì Timing: Real: 84.60s, CPU: 63.42s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:10<00:00,  1.41it/s]\n",
      "\n",
      "‚úì Result: 22/100 correct = 22.00%\n",
      "‚ö†Ô∏è  Invalid responses: 1/100 (1.0%)\n",
      "‚úì Accuracy on valid responses: 22/99 = 22.22%\n",
      "‚úì Timing: Real: 70.70s, CPU: 49.70s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [01:47<00:00,  2.46it/s]\n",
      "\n",
      "‚úì Result: 75/265 correct = 28.30%\n",
      "‚úì Timing: Real: 107.43s, CPU: 73.17s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [01:31<00:00,  1.57it/s]\n",
      "\n",
      "‚úì Result: 29/144 correct = 20.14%\n",
      "‚úì Timing: Real: 91.59s, CPU: 68.24s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:58<00:00,  1.70it/s]\n",
      "\n",
      "‚úì Result: 32/100 correct = 32.00%\n",
      "‚ö†Ô∏è  Invalid responses: 2/100 (2.0%)\n",
      "‚úì Accuracy on valid responses: 32/98 = 32.65%\n",
      "‚úì Timing: Real: 58.54s, CPU: 39.38s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [02:28<00:00,  1.48s/it]\n",
      "\n",
      "‚úì Result: 24/100 correct = 24.00%\n",
      "‚ö†Ô∏è  Invalid responses: 4/100 (4.0%)\n",
      "‚úì Accuracy on valid responses: 24/96 = 25.00%\n",
      "‚úì Timing: Real: 147.57s, CPU: 92.81s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [01:17<00:00,  1.29it/s]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 5/100 (5.0%)\n",
      "‚úì Accuracy on valid responses: 29/95 = 30.53%\n",
      "‚úì Timing: Real: 77.22s, CPU: 41.13s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [03:56<00:00,  1.37s/it]\n",
      "\n",
      "‚úì Result: 37/173 correct = 21.39%\n",
      "‚ö†Ô∏è  Invalid responses: 1/173 (0.6%)\n",
      "‚úì Accuracy on valid responses: 37/172 = 21.51%\n",
      "‚úì Timing: Real: 235.21s, CPU: 124.62s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "‚úì Tokenizer loaded\n",
      "Loading model on mps...\n",
      "‚úì Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [23:38<00:00, 14.19s/it]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 1/100 (1.0%)\n",
      "‚úì Accuracy on valid responses: 29/99 = 29.29%\n",
      "‚úì Timing: Real: 1412.40s, CPU: 534.27s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 135/135 [29:38<00:00, 13.18s/it]\n",
      "\n",
      "‚úì Result: 59/135 correct = 43.70%\n",
      "‚úì Timing: Real: 1771.67s, CPU: 661.61s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 152/152 [26:28<00:00, 10.45s/it]\n",
      "\n",
      "‚úì Result: 66/152 correct = 43.42%\n",
      "‚úì Timing: Real: 1580.78s, CPU: 639.82s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [27:05<00:00, 16.25s/it]\n",
      "\n",
      "‚úì Result: 51/100 correct = 51.00%\n",
      "‚úì Timing: Real: 1617.96s, CPU: 498.72s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 265/265 [52:56<00:00, 11.99s/it]\n",
      "\n",
      "‚úì Result: 123/265 correct = 46.42%\n",
      "‚úì Timing: Real: 3160.23s, CPU: 1096.61s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 144/144 [35:40<00:00, 14.86s/it]\n",
      "\n",
      "‚úì Result: 62/144 correct = 43.06%\n",
      "‚ö†Ô∏è  Invalid responses: 1/144 (0.7%)\n",
      "‚úì Accuracy on valid responses: 62/143 = 43.36%\n",
      "‚úì Timing: Real: 2131.40s, CPU: 713.62s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [24:39<00:00, 14.80s/it]\n",
      "\n",
      "‚úì Result: 28/100 correct = 28.00%\n",
      "‚úì Timing: Real: 1472.82s, CPU: 449.38s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [19:09<00:00, 11.50s/it]\n",
      "\n",
      "‚úì Result: 30/100 correct = 30.00%\n",
      "‚ö†Ô∏è  Invalid responses: 2/100 (2.0%)\n",
      "‚úì Accuracy on valid responses: 30/98 = 30.61%\n",
      "‚úì Timing: Real: 1144.38s, CPU: 426.94s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [09:39<00:00,  5.80s/it]\n",
      "\n",
      "‚úì Result: 29/100 correct = 29.00%\n",
      "‚ö†Ô∏è  Invalid responses: 12/100 (12.0%)\n",
      "‚úì Accuracy on valid responses: 29/88 = 32.95%\n",
      "‚úì Timing: Real: 575.84s, CPU: 235.34s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 173/173 [18:05<00:00,  6.28s/it]\n",
      "\n",
      "‚úì Result: 71/173 correct = 41.04%\n",
      "‚úì Timing: Real: 1080.22s, CPU: 448.92s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "MULTI-MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Device: mps\n",
      "None (full precision)\n",
      "Total Subjects: 10\n",
      "Total Duration: 295.6 minutes\n",
      "======================================================================\n",
      "\n",
      "meta-llama/Llama-3.2-1B-Instruct:\n",
      "  Overall Accuracy: 43.17% (591/1369)\n",
      "  Timing:\n",
      "    Real Time: 514.74s (8.6 min)\n",
      "    CPU Time:  343.74s (5.7 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0:\n",
      "  Overall Accuracy: 24.03% (329/1369)\n",
      "  Invalid Responses: 13/1369 (0.9%)\n",
      "  Accuracy on Valid: 24.26% (329/1356)\n",
      "  Timing:\n",
      "    Real Time: 1057.01s (17.6 min)\n",
      "    CPU Time:  691.65s (11.5 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "Qwen/Qwen2.5-0.5B-Instruct:\n",
      "  Overall Accuracy: 40.03% (548/1369)\n",
      "  Invalid Responses: 16/1369 (1.2%)\n",
      "  Accuracy on Valid: 40.50% (548/1353)\n",
      "  Timing:\n",
      "    Real Time: 15947.69s (265.8 min)\n",
      "    CPU Time:  5705.24s (95.1 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "======================================================================\n",
      "\n",
      "‚úì Results saved to: multi_model_mmlu_results_full_20260225_013843.json\n",
      "\n",
      "üìä Top 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. clinical_knowledge: 54.34%\n",
      "  2. college_biology: 52.78%\n",
      "  3. astronomy: 50.00%\n",
      "\n",
      "üìâ Bottom 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. college_computer_science: 25.00%\n",
      "  2. abstract_algebra: 24.00%\n",
      "  3. college_mathematics: 24.00%\n",
      "\n",
      "üìä Top 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_chemistry: 32.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. clinical_knowledge: 28.30%\n",
      "\n",
      "üìâ Bottom 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_medicine: 21.39%\n",
      "  2. college_biology: 20.14%\n",
      "  3. abstract_algebra: 15.00%\n",
      "\n",
      "üìä Top 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. business_ethics: 51.00%\n",
      "  2. clinical_knowledge: 46.42%\n",
      "  3. anatomy: 43.70%\n",
      "\n",
      "üìâ Bottom 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. abstract_algebra: 29.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. college_chemistry: 28.00%\n",
      "\n",
      "‚úÖ Evaluation complete!\n",
      "real 17790.59\n",
      "user 1134.37\n",
      "sys 5669.05\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/updated_timing_modified_mmlu_eval.txt /usr/bin/time -p python modified_mmlu_eval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2223200",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16651) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remote: Enumerating objects: 59, done.\u001b[K\n",
      "remote: Counting objects: 100% (59/59), done.\u001b[K\n",
      "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
      "remote: Total 48 (delta 35), reused 21 (delta 15), pack-reused 0 (from 0)\u001b[K\n",
      "Unpacking objects: 100% (48/48), 41.38 KiB | 847.00 KiB/s, done.\n",
      "From https://github.com/shainakumar/cs6501workshop\n",
      "   5074523..e0344b8  main       -> origin/main\n",
      "Updating 5074523..e0344b8\n",
      "Fast-forward\n",
      " Topic3Tools/README.md                              |  75 \u001b[32m++\u001b[m\n",
      " Topic3Tools/outputs/lg_graph.png                   | Bin \u001b[31m0\u001b[m -> \u001b[32m19449\u001b[m bytes\n",
      " Topic3Tools/outputs/task1_parallel_execution.txt   | 764 \u001b[32m++++++++++++++\u001b[m\u001b[31m-------\u001b[m\n",
      " Topic3Tools/outputs/task1_sequential_execution.txt | 764 \u001b[32m++++++++++++++\u001b[m\u001b[31m-------\u001b[m\n",
      " Topic3Tools/outputs/task1_unmodified_timing_s1.txt |  45 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " Topic3Tools/outputs/task1_unmodified_timing_s2.txt |  32 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../task3_manual_tool_handling_geo_calc.txt        |  30 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
      " .../outputs/task4_langgraph_multiple_tools2.txt    | 127 \u001b[32m+++\u001b[m\u001b[31m-\u001b[m\n",
      " .../outputs/task5_recovered_single_long_convo.txt  |   0\n",
      " Topic3Tools/outputs/task5_single_long_convo.txt    |   0\n",
      " Topic3Tools/task3_manual_tool_handling_geo_calc.py | 151 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
      " Topic3Tools/task4_multiple_tool_use.py             | 132 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
      " Topic3Tools/task5_single_long_convo.py             | 397 \u001b[32m++++++\u001b[m\u001b[31m-----\u001b[m\n",
      " 13 files changed, 1635 insertions(+), 882 deletions(-)\n",
      " create mode 100644 Topic3Tools/README.md\n",
      " create mode 100644 Topic3Tools/outputs/lg_graph.png\n",
      " create mode 100644 Topic3Tools/outputs/task5_recovered_single_long_convo.txt\n",
      " create mode 100644 Topic3Tools/outputs/task5_single_long_convo.txt\n"
     ]
    }
   ],
   "source": [
    "!git pull \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80cc9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add . \n",
    "!git commit -m\"rerun local\" \n",
    "!git push"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
