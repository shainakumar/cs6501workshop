{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "376ae2d9",
   "metadata": {},
   "source": [
    "# Create a Python environment with the following modules installed by conda or pip:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bf097ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.57.5)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.4.2)\n",
      "Requirement already satisfied: accelerate in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (1.12.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: bitsandbytes in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (0.49.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (2025.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (4.15.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from huggingface_hub) (1.2.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from datasets) (0.70.18)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (4.12.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (2026.1.4)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpx<1.0.0->datasets) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from accelerate) (7.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2.6.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets) (2025.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers torch datasets accelerate tqdm huggingface_hub bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33142549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipywidgets\n",
      "  Downloading ipywidgets-8.1.8-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (0.2.3)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (9.9.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Collecting widgetsnbextension~=4.0.14 (from ipywidgets)\n",
      "  Downloading widgetsnbextension-4.0.15-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting jupyterlab_widgets~=3.0.15 (from ipywidgets)\n",
      "  Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: decorator>=4.3.2 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers>=1.0.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.18.1 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.11.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.19.2)\n",
      "Requirement already satisfied: stack_data>=0.6.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.6.3)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.5.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from jedi>=0.18.1->ipython>=6.1.0->ipywidgets) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (3.0.1)\n",
      "Requirement already satisfied: pure_eval in /opt/anaconda3/envs/myenv/lib/python3.12/site-packages (from stack_data>=0.6.0->ipython>=6.1.0->ipywidgets) (0.2.3)\n",
      "Downloading ipywidgets-8.1.8-py3-none-any.whl (139 kB)\n",
      "Downloading jupyterlab_widgets-3.0.16-py3-none-any.whl (914 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m914.9/914.9 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.15-py3-none-any.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, jupyterlab_widgets, ipywidgets\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3/3\u001b[0m [ipywidgets]3\u001b[0m [ipywidgets]\n",
      "\u001b[1A\u001b[2KSuccessfully installed ipywidgets-8.1.8 jupyterlab_widgets-3.0.16 widgetsnbextension-4.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03d28f2",
   "metadata": {},
   "source": [
    "# Set up your computing environment with Hugging Face authorization for Llama 3.2-1B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "caeb0049",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43635da0e40f460890cf296b13871035",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1muser: \u001b[0m shainakumar\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login  \n",
    "notebook_login()\n",
    "!hf auth whoami "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae11fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p outputs plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28762625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "macOS-15.6-arm64-arm-64bit\n",
      "torch 2.5.1\n",
      "cuda False\n",
      "mps True\n"
     ]
    }
   ],
   "source": [
    "!python -c \"import platform, torch; print(platform.platform()); print('torch', torch.__version__); print('cuda', torch.cuda.is_available()); print('mps', hasattr(torch.backends,'mps') and torch.backends.mps.is_available())\" \\\n",
    "| tee outputs/env_info.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3004e9b4",
   "metadata": {},
   "source": [
    "# Verify that your setup is working by running llama_mmlu_eval.py which runs the model on two MMLU topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b617cb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:42<00:00,  1.48it/s]\n",
      "âœ“ Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:58<00:00,  1.72it/s]\n",
      "âœ“ Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.7 minutes\n",
      "======================================================================\n",
      "\n",
      "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "\n",
      "ğŸ“Š Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "ğŸ“‰ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "âœ… Evaluation complete!\n",
      "real 192.80\n",
      "user 81.59\n",
      "sys 40.90\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/baseline_mps_noquant.txt /usr/bin/time -p python llama_mmlu_eval.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f1deb78",
   "metadata": {},
   "source": [
    "# Time the code using the time shell command line function. \n",
    "\n",
    "Compare the timings for the following setups:\n",
    "\n",
    "Using GPU and no quantization.\n",
    "\n",
    "Using GPU and 4-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using GPU and 8-bit quantization. (Not possible on a MacBook, skip if that is your laptop.)\n",
    "\n",
    "Using CPU and no quantization.\n",
    "\n",
    "Using CPU and 4-bit quantization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "649c5f52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic1LLMs.ipynb\n",
      "eval_cpu_noquant.py\n",
      "eval_mps_noquant.py\n",
      "llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      "llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "llama_mmlu_eval.py\n",
      "\u001b[34moutputs\u001b[m\u001b[m\n",
      "\u001b[34mplots\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!cp llama_mmlu_eval.py eval_mps_noquant.py\n",
    "!cp llama_mmlu_eval.py eval_cpu_noquant.py\n",
    "!mkdir -p outputs\n",
    "!ls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2875102d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Llama 3.2-1B MMLU Evaluation (Quantized)\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory: ~2.5 GB (FP16)\n",
      "Number of subjects: 2\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Loading model meta-llama/Llama-3.2-1B-Instruct...\n",
      "Device: mps\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model (this may take 2-3 minutes)...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 2 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:29<00:00,  1.70it/s]\n",
      "âœ“ Result: 76/152 correct = 50.00%\n",
      "\n",
      "Progress: 2/2 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:48<00:00,  2.04it/s]\n",
      "âœ“ Result: 45/100 correct = 45.00%\n",
      "\n",
      "======================================================================\n",
      "EVALUATION SUMMARY\n",
      "======================================================================\n",
      "Model: meta-llama/Llama-3.2-1B-Instruct\n",
      "None (full precision)\n",
      "Total Subjects: 2\n",
      "Total Questions: 252\n",
      "Total Correct: 121\n",
      "Overall Accuracy: 48.02%\n",
      "Duration: 2.3 minutes\n",
      "======================================================================\n",
      "\n",
      "âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      "\n",
      "ğŸ“Š Top 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "ğŸ“‰ Bottom 5 Subjects:\n",
      "  1. astronomy: 50.00%\n",
      "  2. business_ethics: 45.00%\n",
      "\n",
      "âœ… Evaluation complete!\n",
      "real 170.45\n",
      "user 72.62\n",
      "sys 33.63\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_mps_noquant.txt /usr/bin/time -p python eval_mps_noquant.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "360b7b8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Untracked files:\n",
      "  (use \"git add <file>...\" to include in what will be committed)\n",
      "\t\u001b[31mTopic1LLMs.ipynb\u001b[m\n",
      "\t\u001b[31meval_cpu_noquant.py\u001b[m\n",
      "\t\u001b[31meval_mps_noquant.py\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_212449.json\u001b[m\n",
      "\t\u001b[31mllama_3.2_1b_mmlu_results_full_20260131_213254.json\u001b[m\n",
      "\t\u001b[31mllama_mmlu_eval.py\u001b[m\n",
      "\t\u001b[31moutputs/baseline_mps_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/env_info.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_cpu_noquant.txt\u001b[m\n",
      "\t\u001b[31moutputs/timing_mps_noquant.txt\u001b[m\n",
      "\n",
      "nothing added to commit but untracked files present (use \"git add\" to track)\n"
     ]
    }
   ],
   "source": [
    "!git status --untracked-files=all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c5541a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is up to date with 'origin/main'.\n",
      "\n",
      "Changes to be committed:\n",
      "  (use \"git restore --staged <file>...\" to unstage)\n",
      "\t\u001b[32mnew file:   Topic1LLMs.ipynb\u001b[m\n",
      "\t\u001b[32mnew file:   eval_cpu_noquant.py\u001b[m\n",
      "\t\u001b[32mnew file:   eval_mps_noquant.py\u001b[m\n",
      "\t\u001b[32mnew file:   llama_3.2_1b_mmlu_results_full_20260131_212449.json\u001b[m\n",
      "\t\u001b[32mnew file:   llama_3.2_1b_mmlu_results_full_20260131_213254.json\u001b[m\n",
      "\t\u001b[32mnew file:   llama_mmlu_eval.py\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/baseline_mps_noquant.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/env_info.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/timing_cpu_noquant.txt\u001b[m\n",
      "\t\u001b[32mnew file:   outputs/timing_mps_noquant.txt\u001b[m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!git add .\n",
    "!git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fa5524f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main ecd4657] baseline timings and setup\n",
      " 10 files changed, 2415 insertions(+)\n",
      " create mode 100644 Running_an_LLM/Topic1LLMs.ipynb\n",
      " create mode 100644 Running_an_LLM/eval_cpu_noquant.py\n",
      " create mode 100644 Running_an_LLM/eval_mps_noquant.py\n",
      " create mode 100644 Running_an_LLM/llama_3.2_1b_mmlu_results_full_20260131_212449.json\n",
      " create mode 100644 Running_an_LLM/llama_3.2_1b_mmlu_results_full_20260131_213254.json\n",
      " create mode 100644 Running_an_LLM/llama_mmlu_eval.py\n",
      " create mode 100644 Running_an_LLM/outputs/baseline_mps_noquant.txt\n",
      " create mode 100644 Running_an_LLM/outputs/env_info.txt\n",
      " create mode 100644 Running_an_LLM/outputs/timing_cpu_noquant.txt\n",
      " create mode 100644 Running_an_LLM/outputs/timing_mps_noquant.txt\n"
     ]
    }
   ],
   "source": [
    "!git commit -m \"baseline timings and setup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a0fd9a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enumerating objects: 14, done.\n",
      "Counting objects: 100% (14/14), done.\n",
      "Delta compression using up to 8 threads\n",
      "Compressing objects: 100% (13/13), done.\n",
      "Writing objects: 100% (13/13), 22.30 KiB | 7.43 MiB/s, done.\n",
      "Total 13 (delta 3), reused 0 (delta 0), pack-reused 0\n",
      "remote: Resolving deltas: 100% (3/3), done.\u001b[K\n",
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      "   12116bb..ecd4657  main -> main\n"
     ]
    }
   ],
   "source": [
    "!git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe6163e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!script -q outputs/timing_cpu_noquant.txt /usr/bin/time -p python eval_cpu_noquant.py\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f7910a",
   "metadata": {},
   "source": [
    "# Modify the code to do the following:\n",
    "\n",
    "Run on a selection of 10 subjects using 2 other tiny/small models in addtion to Llama 3.2-1B.  \n",
    "\n",
    "Add timing information to the evaluation summary showing the cycles consumed by each model.  Include all of real time, CPU time, and GPU time.\n",
    "\n",
    "Add an option to the program to make it print out each question, the answer the model gives, and whether the answer is right or wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81d37050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp llama_mmlu_eval.py modified_mmlu_eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c79aa314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Multi-Model MMLU Evaluation\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "Environment Check\n",
      "======================================================================\n",
      "âœ“ Running locally (not in Colab)\n",
      "âœ“ Platform: Darwin (arm64)\n",
      "âœ“ Processor: arm\n",
      "âœ“ Apple Metal (MPS) Available\n",
      "âœ“ Using Metal Performance Shaders for GPU acceleration\n",
      "âœ“ Quantization disabled - loading full precision model\n",
      "âœ“ Hugging Face authenticated\n",
      "\n",
      "======================================================================\n",
      "Configuration\n",
      "======================================================================\n",
      "Models: 3\n",
      "  - meta-llama/Llama-3.2-1B-Instruct\n",
      "  - TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "  - Qwen/Qwen2.5-0.5B-Instruct\n",
      "Device: mps\n",
      "Quantization: None (full precision)\n",
      "Expected memory per model: ~2.5 GB (FP16)\n",
      "Number of subjects: 10\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: meta-llama/Llama-3.2-1B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model on mps...\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:18<00:00,  1.28it/s]\n",
      "\n",
      "âœ“ Result: 24/100 correct = 24.00%\n",
      "âœ“ Timing: Real: 77.60s, CPU: 51.25s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [01:36<00:00,  1.40it/s]\n",
      "\n",
      "âœ“ Result: 65/135 correct = 48.15%\n",
      "âœ“ Timing: Real: 95.47s, CPU: 64.46s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [01:16<00:00,  2.00it/s]\n",
      "\n",
      "âœ“ Result: 76/152 correct = 50.00%\n",
      "âœ“ Timing: Real: 75.91s, CPU: 49.60s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:46<00:00,  2.15it/s]\n",
      "\n",
      "âœ“ Result: 45/100 correct = 45.00%\n",
      "âœ“ Timing: Real: 46.38s, CPU: 30.11s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265/265 [01:39<00:00,  2.67it/s]\n",
      "\n",
      "âœ“ Result: 144/265 correct = 54.34%\n",
      "âœ“ Timing: Real: 98.85s, CPU: 65.72s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [01:12<00:00,  1.99it/s]\n",
      "\n",
      "âœ“ Result: 76/144 correct = 52.78%\n",
      "âœ“ Timing: Real: 72.11s, CPU: 44.28s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:09<00:00,  1.43it/s]\n",
      "\n",
      "âœ“ Result: 33/100 correct = 33.00%\n",
      "âœ“ Timing: Real: 69.49s, CPU: 47.65s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [01:29<00:00,  1.12it/s]\n",
      "\n",
      "âœ“ Result: 25/100 correct = 25.00%\n",
      "âœ“ Timing: Real: 89.39s, CPU: 55.81s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [00:55<00:00,  1.81it/s]\n",
      "\n",
      "âœ“ Result: 24/100 correct = 24.00%\n",
      "âœ“ Timing: Real: 54.99s, CPU: 30.68s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [01:50<00:00,  1.56it/s]\n",
      "\n",
      "âœ“ Result: 79/173 correct = 45.66%\n",
      "âœ“ Timing: Real: 110.44s, CPU: 60.28s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "tokenizer_config.json: 1.29kB [00:00, 418kB/s]\n",
      "tokenizer.model: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 500k/500k [00:01<00:00, 415kB/s]\n",
      "tokenizer.json: 1.84MB [00:00, 19.6MB/s]\n",
      "special_tokens_map.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 551/551 [00:00<00:00, 2.38MB/s]\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model on mps...\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 608/608 [00:00<00:00, 6.34MB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.20G/2.20G [01:41<00:00, 21.6MB/s]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 124/124 [00:00<00:00, 1.15MB/s]\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [08:31<00:00,  5.12s/it]\n",
      "\n",
      "âœ“ Result: 15/100 correct = 15.00%\n",
      "âœ“ Timing: Real: 509.39s, CPU: 237.01s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [05:10<00:00,  2.30s/it]\n",
      "\n",
      "âœ“ Result: 32/135 correct = 23.70%\n",
      "âœ“ Timing: Real: 308.88s, CPU: 163.33s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [03:09<00:00,  1.24s/it]\n",
      "\n",
      "âœ“ Result: 34/152 correct = 22.37%\n",
      "âœ“ Timing: Real: 187.99s, CPU: 115.56s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [03:09<00:00,  1.89s/it]\n",
      "\n",
      "âœ“ Result: 22/100 correct = 22.00%\n",
      "âš ï¸  Invalid responses: 1/100 (1.0%)\n",
      "âœ“ Accuracy on valid responses: 22/99 = 22.22%\n",
      "âœ“ Timing: Real: 187.98s, CPU: 85.45s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265/265 [07:20<00:00,  1.66s/it]\n",
      "\n",
      "âœ“ Result: 75/265 correct = 28.30%\n",
      "âœ“ Timing: Real: 437.11s, CPU: 184.44s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [04:57<00:00,  2.07s/it]\n",
      "\n",
      "âœ“ Result: 29/144 correct = 20.14%\n",
      "âœ“ Timing: Real: 296.03s, CPU: 149.91s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [04:09<00:00,  2.50s/it]\n",
      "\n",
      "âœ“ Result: 32/100 correct = 32.00%\n",
      "âš ï¸  Invalid responses: 2/100 (2.0%)\n",
      "âœ“ Accuracy on valid responses: 32/98 = 32.65%\n",
      "âœ“ Timing: Real: 248.50s, CPU: 105.19s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [05:59<00:00,  3.60s/it]\n",
      "\n",
      "âœ“ Result: 24/100 correct = 24.00%\n",
      "âš ï¸  Invalid responses: 4/100 (4.0%)\n",
      "âœ“ Accuracy on valid responses: 24/96 = 25.00%\n",
      "âœ“ Timing: Real: 358.56s, CPU: 168.21s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [02:21<00:00,  1.41s/it]\n",
      "\n",
      "âœ“ Result: 29/100 correct = 29.00%\n",
      "âš ï¸  Invalid responses: 5/100 (5.0%)\n",
      "âœ“ Accuracy on valid responses: 29/95 = 30.53%\n",
      "âœ“ Timing: Real: 140.63s, CPU: 66.05s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [12:53<00:00,  4.47s/it]\n",
      "\n",
      "âœ“ Result: 37/173 correct = 21.39%\n",
      "âš ï¸  Invalid responses: 1/173 (0.6%)\n",
      "âœ“ Accuracy on valid responses: 37/172 = 21.51%\n",
      "âœ“ Timing: Real: 766.90s, CPU: 295.09s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODEL: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "\n",
      "\n",
      "======================================================================\n",
      "Loading model: Qwen/Qwen2.5-0.5B-Instruct\n",
      "======================================================================\n",
      "Loading tokenizer...\n",
      "tokenizer_config.json: 7.30kB [00:00, 520kB/s]\n",
      "vocab.json: 2.78MB [00:00, 13.0MB/s]\n",
      "merges.txt: 1.67MB [00:00, 13.4MB/s]\n",
      "tokenizer.json: 7.03MB [00:00, 20.4MB/s]\n",
      "âœ“ Tokenizer loaded\n",
      "Loading model on mps...\n",
      "config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 659/659 [00:00<00:00, 1.72MB/s]\n",
      "model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 988M/988M [00:44<00:00, 22.2MB/s]\n",
      "generation_config.json: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 242/242 [00:00<00:00, 2.26MB/s]\n",
      "âœ“ Model loaded successfully!\n",
      "  Model device: mps:0\n",
      "  Model dtype: torch.float16\n",
      "  Running on Apple Metal (MPS)\n",
      "\n",
      "======================================================================\n",
      "Starting evaluation on 10 subjects\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Progress: 1/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: abstract_algebra\n",
      "======================================================================\n",
      "Testing abstract_algebra:   0%|                         | 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p', 'top_k']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Testing abstract_algebra: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [21:34<00:00, 12.95s/it]\n",
      "\n",
      "âœ“ Result: 29/100 correct = 29.00%\n",
      "âš ï¸  Invalid responses: 1/100 (1.0%)\n",
      "âœ“ Accuracy on valid responses: 29/99 = 29.29%\n",
      "âœ“ Timing: Real: 1289.31s, CPU: 551.10s, GPU: 0.00s\n",
      "\n",
      "Progress: 2/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: anatomy\n",
      "======================================================================\n",
      "Testing anatomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 135/135 [25:34<00:00, 11.36s/it]\n",
      "\n",
      "âœ“ Result: 59/135 correct = 43.70%\n",
      "âœ“ Timing: Real: 1526.67s, CPU: 548.35s, GPU: 0.00s\n",
      "\n",
      "Progress: 3/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: astronomy\n",
      "======================================================================\n",
      "Testing astronomy: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 152/152 [18:41<00:00,  7.38s/it]\n",
      "\n",
      "âœ“ Result: 66/152 correct = 43.42%\n",
      "âœ“ Timing: Real: 1116.14s, CPU: 427.72s, GPU: 0.00s\n",
      "\n",
      "Progress: 4/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: business_ethics\n",
      "======================================================================\n",
      "Testing business_ethics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:47<00:00,  5.87s/it]\n",
      "\n",
      "âœ“ Result: 51/100 correct = 51.00%\n",
      "âœ“ Timing: Real: 583.99s, CPU: 229.51s, GPU: 0.00s\n",
      "\n",
      "Progress: 5/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: clinical_knowledge\n",
      "======================================================================\n",
      "Testing clinical_knowledge: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 265/265 [16:47<00:00,  3.80s/it]\n",
      "\n",
      "âœ“ Result: 123/265 correct = 46.42%\n",
      "âœ“ Timing: Real: 999.46s, CPU: 407.58s, GPU: 0.00s\n",
      "\n",
      "Progress: 6/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_biology\n",
      "======================================================================\n",
      "Testing college_biology: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144/144 [14:42<00:00,  6.13s/it]\n",
      "\n",
      "âœ“ Result: 62/144 correct = 43.06%\n",
      "âš ï¸  Invalid responses: 1/144 (0.7%)\n",
      "âœ“ Accuracy on valid responses: 62/143 = 43.36%\n",
      "âœ“ Timing: Real: 878.63s, CPU: 345.76s, GPU: 0.00s\n",
      "\n",
      "Progress: 7/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_chemistry\n",
      "======================================================================\n",
      "Testing college_chemistry: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [09:50<00:00,  5.90s/it]\n",
      "\n",
      "âœ“ Result: 28/100 correct = 28.00%\n",
      "âœ“ Timing: Real: 587.23s, CPU: 240.45s, GPU: 0.00s\n",
      "\n",
      "Progress: 8/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_computer_science\n",
      "======================================================================\n",
      "Testing college_computer_science: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [16:40<00:00, 10.01s/it]\n",
      "\n",
      "âœ“ Result: 30/100 correct = 30.00%\n",
      "âš ï¸  Invalid responses: 2/100 (2.0%)\n",
      "âœ“ Accuracy on valid responses: 30/98 = 30.61%\n",
      "âœ“ Timing: Real: 996.22s, CPU: 365.57s, GPU: 0.00s\n",
      "\n",
      "Progress: 9/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_mathematics\n",
      "======================================================================\n",
      "Testing college_mathematics: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 100/100 [06:56<00:00,  4.17s/it]\n",
      "\n",
      "âœ“ Result: 29/100 correct = 29.00%\n",
      "âš ï¸  Invalid responses: 12/100 (12.0%)\n",
      "âœ“ Accuracy on valid responses: 29/88 = 32.95%\n",
      "âœ“ Timing: Real: 414.97s, CPU: 166.37s, GPU: 0.00s\n",
      "\n",
      "Progress: 10/10 subjects\n",
      "\n",
      "======================================================================\n",
      "Evaluating subject: college_medicine\n",
      "======================================================================\n",
      "Testing college_medicine: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 173/173 [22:56<00:00,  7.96s/it]\n",
      "\n",
      "âœ“ Result: 71/173 correct = 41.04%\n",
      "âœ“ Timing: Real: 1370.58s, CPU: 505.21s, GPU: 0.00s\n",
      "\n",
      "======================================================================\n",
      "MULTI-MODEL COMPARISON SUMMARY\n",
      "======================================================================\n",
      "Device: mps\n",
      "None (full precision)\n",
      "Total Subjects: 10\n",
      "Total Duration: 240.4 minutes\n",
      "======================================================================\n",
      "\n",
      "meta-llama/Llama-3.2-1B-Instruct:\n",
      "  Overall Accuracy: 43.17% (591/1369)\n",
      "  Timing:\n",
      "    Real Time: 790.64s (13.2 min)\n",
      "    CPU Time:  499.85s (8.3 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "TinyLlama/TinyLlama-1.1B-Chat-v1.0:\n",
      "  Overall Accuracy: 24.03% (329/1369)\n",
      "  Invalid Responses: 13/1369 (0.9%)\n",
      "  Accuracy on Valid: 24.26% (329/1356)\n",
      "  Timing:\n",
      "    Real Time: 3441.96s (57.4 min)\n",
      "    CPU Time:  1570.24s (26.2 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "\n",
      "Qwen/Qwen2.5-0.5B-Instruct:\n",
      "  Overall Accuracy: 40.03% (548/1369)\n",
      "  Invalid Responses: 16/1369 (1.2%)\n",
      "  Accuracy on Valid: 40.50% (548/1353)\n",
      "  Timing:\n",
      "    Real Time: 9763.19s (162.7 min)\n",
      "    CPU Time:  3787.63s (63.1 min)\n",
      "    GPU Time:  0.00s (0.0 min)\n",
      "======================================================================\n",
      "\n",
      "âœ“ Results saved to: multi_model_mmlu_results_full_20260201_040616.json\n",
      "\n",
      "ğŸ“Š Top 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. clinical_knowledge: 54.34%\n",
      "  2. college_biology: 52.78%\n",
      "  3. astronomy: 50.00%\n",
      "\n",
      "ğŸ“‰ Bottom 3 Subjects for Llama-3.2-1B-Instruct:\n",
      "  1. college_computer_science: 25.00%\n",
      "  2. abstract_algebra: 24.00%\n",
      "  3. college_mathematics: 24.00%\n",
      "\n",
      "ğŸ“Š Top 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_chemistry: 32.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. clinical_knowledge: 28.30%\n",
      "\n",
      "ğŸ“‰ Bottom 3 Subjects for TinyLlama-1.1B-Chat-v1.0:\n",
      "  1. college_medicine: 21.39%\n",
      "  2. college_biology: 20.14%\n",
      "  3. abstract_algebra: 15.00%\n",
      "\n",
      "ğŸ“Š Top 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. business_ethics: 51.00%\n",
      "  2. clinical_knowledge: 46.42%\n",
      "  3. anatomy: 43.70%\n",
      "\n",
      "ğŸ“‰ Bottom 3 Subjects for Qwen2.5-0.5B-Instruct:\n",
      "  1. abstract_algebra: 29.00%\n",
      "  2. college_mathematics: 29.00%\n",
      "  3. college_chemistry: 28.00%\n",
      "\n",
      "âœ… Evaluation complete!\n",
      "real 14488.82\n",
      "user 1267.85\n",
      "sys 4697.66\n"
     ]
    }
   ],
   "source": [
    "!script -q outputs/timing_modified_mmlu_eval.txt /usr/bin/time -p python modified_mmlu_eval.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad3aa959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17470) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17475) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[main 29f83be] local multi model eval\n",
      " 5 files changed, 2200 insertions(+), 53 deletions(-)\n",
      " create mode 100644 Running_an_LLM/modified_mmlu_eval.py\n",
      " create mode 100644 Running_an_LLM/multi_model_mmlu_results_full_20260201_040616.json\n",
      " create mode 100644 Running_an_LLM/outputs/timing_modified_mmlu_eval.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17477) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/shainakumar/cs6501workshop.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git add . \n",
    "!git commit -m \"local multi model eval\"\n",
    "!git push "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf970112",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17541) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On branch main\n",
      "Your branch is ahead of 'origin/main' by 1 commit.\n",
      "  (use \"git push\" to publish your local commits)\n",
      "\n",
      "Changes not staged for commit:\n",
      "  (use \"git add <file>...\" to update what will be committed)\n",
      "  (use \"git restore <file>...\" to discard changes in working directory)\n",
      "\t\u001b[31mmodified:   Topic1LLMs.ipynb\u001b[m\n",
      "\n",
      "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17542) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: cannot pull with rebase: You have unstaged changes.\n",
      "error: please commit or stash them.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17547) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To https://github.com/shainakumar/cs6501workshop.git\n",
      " \u001b[31m! [rejected]       \u001b[m main -> main (fetch first)\n",
      "\u001b[31merror: failed to push some refs to 'https://github.com/shainakumar/cs6501workshop.git'\n",
      "\u001b[m\u001b[33mhint: Updates were rejected because the remote contains work that you do\u001b[m\n",
      "\u001b[33mhint: not have locally. This is usually caused by another repository pushing\u001b[m\n",
      "\u001b[33mhint: to the same ref. You may want to first integrate the remote changes\u001b[m\n",
      "\u001b[33mhint: (e.g., 'git pull ...') before pushing again.\u001b[m\n",
      "\u001b[33mhint: See the 'Note about fast-forwards' in 'git push --help' for details.\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!git status\n",
    "!git pull --rebase origin main\n",
    "!git push origin main\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f75ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) stash the notebook change \n",
    "!git stash push -m \"WIP Topic1LLMs notebook\" -- Topic1LLMs.ipynb\n",
    "\n",
    "# 2) integrate remote changes cleanly\n",
    "!git pull --rebase origin main\n",
    "\n",
    "# 3) push your existing commit(s)\n",
    "!git push origin main\n",
    "\n",
    "# 4) restore your notebook change\n",
    "!git stash pop\n",
    "\n",
    "# 5) commit the notebook change and push it too\n",
    "!git add Topic1LLMs.ipynb\n",
    "!git commit -m \"Update Topic1LLMs notebook\"\n",
    "!git push origin main\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
