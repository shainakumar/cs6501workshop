
======================================================================
Llama 3.2-1B MMLU Evaluation (Quantized)
======================================================================

======================================================================
Environment Check
======================================================================
✓ Running locally (not in Colab)
✓ Platform: Darwin (arm64)
✓ Processor: arm
⚠️  No GPU detected - running on CPU
✓ Quantization disabled - loading full precision model
✓ Hugging Face authenticated

======================================================================
Configuration
======================================================================
Model: meta-llama/Llama-3.2-1B-Instruct
Device: cpu
Quantization: None (full precision)
Expected memory: ~5 GB (FP32)
Number of subjects: 2
======================================================================


Loading model meta-llama/Llama-3.2-1B-Instruct...
Device: cpu
✓ Tokenizer loaded
Loading model (this may take 2-3 minutes)...
✓ Model loaded successfully!
  Model device: cpu
  Model dtype: torch.float32

======================================================================
Starting evaluation on 2 subjects
======================================================================


Progress: 1/2 subjects

======================================================================
Evaluating subject: astronomy
======================================================================
Testing astronomy:   0%|                                | 0/152 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Testing astronomy:   1%|▏                     | 1/152 [00:28<1:11:51, 28.55s/it]Testing astronomy:   1%|▎                     | 2/152 [01:13<1:36:00, 38.41s/it]