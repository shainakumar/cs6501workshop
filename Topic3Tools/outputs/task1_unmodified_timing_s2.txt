Script started on 2026-01-28 01:31:53+00:00 [TERM="xterm-color" TTY="/dev/pts/0" COLUMNS="-1" LINES="-1"]

======================================================================
Llama 3.2-1B MMLU Evaluation (Quantized)
======================================================================

======================================================================
Environment Check
======================================================================
âœ“ Running in Google Colab
âœ“ Platform: Linux (x86_64)
âœ“ GPU Available: Tesla T4
âœ“ GPU Memory: 15.83 GB
âœ“ Quantization disabled - loading full precision model
âœ“ Hugging Face authenticated

======================================================================
Configuration
======================================================================
Model: meta-llama/Llama-3.2-1B-Instruct
Device: cuda
Quantization: None (full precision)
Expected memory: ~2.5 GB (FP16)
Number of subjects: 1
======================================================================


Loading model meta-llama/Llama-3.2-1B-Instruct...
Device: cuda
âœ“ Tokenizer loaded
Loading model (this may take 2-3 minutes)...
2026-01-28 01:32:01.491308: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1769563921.513170    8699 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1769563921.518215    8699 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
W0000 00:00:1769563921.530853    8699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769563921.530874    8699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769563921.530878    8699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
W0000 00:00:1769563921.530884    8699 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.
2026-01-28 01:32:01.534849: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
âœ“ Model loaded successfully!
  Model device: cuda:0
  Model dtype: torch.float16
  GPU Memory: 2.47 GB allocated, 2.51 GB reserved

======================================================================
Starting evaluation on 1 subjects
======================================================================


Progress: 1/1 subjects

======================================================================
Evaluating subject: abstract_algebra
======================================================================
Testing abstract_algebra:   0% 0/100 [00:00<?, ?it/s]The following generation flags are not valid and may be ignored: ['top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.
Testing abstract_algebra:   1% 1/100 [00:00<00:42,  2.32it/s]Testing abstract_algebra:   5% 5/100 [00:00<00:08, 10.97it/s]Testing abstract_algebra:   9% 9/100 [00:00<00:05, 17.32it/s]Testing abstract_algebra:  12% 12/100 [00:00<00:04, 20.15it/s]Testing abstract_algebra:  16% 16/100 [00:00<00:03, 24.68it/s]Testing abstract_algebra:  20% 20/100 [00:01<00:02, 27.84it/s]Testing abstract_algebra:  24% 24/100 [00:01<00:02, 30.40it/s]Testing abstract_algebra:  28% 28/100 [00:01<00:02, 31.72it/s]Testing abstract_algebra:  32% 32/100 [00:01<00:02, 32.75it/s]Testing abstract_algebra:  36% 36/100 [00:01<00:01, 33.44it/s]Testing abstract_algebra:  40% 40/100 [00:01<00:01, 33.34it/s]Testing abstract_algebra:  44% 44/100 [00:01<00:01, 33.40it/s]Testing abstract_algebra:  48% 48/100 [00:01<00:01, 32.94it/s]Testing abstract_algebra:  52% 52/100 [00:01<00:01, 34.05it/s]Testing abstract_algebra:  56% 56/100 [00:02<00:01, 34.05it/s]Testing abstract_algebra:  60% 60/100 [00:02<00:01, 34.88it/s]Testing abstract_algebra:  64% 64/100 [00:02<00:01, 35.32it/s]Testing abstract_algebra:  68% 68/100 [00:02<00:00, 35.60it/s]Testing abstract_algebra:  72% 72/100 [00:02<00:00, 35.98it/s]Testing abstract_algebra:  76% 76/100 [00:02<00:00, 36.16it/s]Testing abstract_algebra:  80% 80/100 [00:02<00:00, 36.15it/s]Testing abstract_algebra:  84% 84/100 [00:02<00:00, 34.92it/s]Testing abstract_algebra:  88% 88/100 [00:02<00:00, 34.91it/s]Testing abstract_algebra:  92% 92/100 [00:03<00:00, 35.53it/s]Testing abstract_algebra:  96% 96/100 [00:03<00:00, 36.08it/s]Testing abstract_algebra: 100% 100/100 [00:03<00:00, 36.58it/s]Testing abstract_algebra: 100% 100/100 [00:03<00:00, 30.60it/s]
âœ“ Result: 24/100 correct = 24.00%

======================================================================
EVALUATION SUMMARY
======================================================================
Model: meta-llama/Llama-3.2-1B-Instruct
None (full precision)
Total Subjects: 1
Total Questions: 100
Total Correct: 24
Overall Accuracy: 24.00%
Duration: 0.1 minutes
======================================================================

âœ“ Results saved to: llama_3.2_1b_mmlu_results_full_20260128_013212.json

ðŸ“Š Top 5 Subjects:
  1. abstract_algebra: 24.00%

ðŸ“‰ Bottom 5 Subjects:
  1. abstract_algebra: 24.00%

======================================================================
ðŸ’¾ To download results in Colab:
======================================================================
from google.colab import files
files.download('llama_3.2_1b_mmlu_results_full_20260128_013212.json')

âœ… Evaluation complete!

real	0m21.119s
user	0m16.767s
sys	0m2.062s

Script done on 2026-01-28 01:32:14+00:00 [COMMAND_EXIT_CODE="0"]
