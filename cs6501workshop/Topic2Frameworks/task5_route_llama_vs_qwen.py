# langgraph_conditional_models.py
# Program demonstrates use of LangGraph with conditional model selection.
# It writes to stdout and asks the user to enter a line of text through stdin.
# If the input starts with "Hey Qwen", it routes to Qwen2.5-0.5B-Instruct,
# otherwise it routes to llama-3.2-1B-Instruct.
# The LLMs should use Cuda if available, if not then if mps is available then use that,
# otherwise use cpu.
# After the LangGraph graph is created but before it executes, the program
# uses the Mermaid library to write a image of the graph to the file lg_graph.png
# The program gets the LLMs from Hugging Face and wraps them for LangChain using HuggingFacePipeline.
# The code is commented in detail so a reader can understand each step.
# Supports verbose mode: type "verbose" to enable tracing, "quiet" to disable.

# Import necessary libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# Determine the best available device for inference
# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
def get_device():
    """
    Detect and return the best available compute device.
    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.
    """
    if torch.cuda.is_available():
        print("Using CUDA (NVIDIA GPU) for inference")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon) for inference")
        return "mps"
    else:
        print("Using CPU for inference")
        return "cpu"

# =============================================================================
# STATE DEFINITION
# =============================================================================
# The state is a TypedDict that flows through all nodes in the graph.
# Each node can read from and write to specific fields in the state.
# LangGraph automatically merges the returned dict from each node into the state.

class AgentState(TypedDict):
    """
    State object that flows through the LangGraph nodes.

    Fields:
    - user_input: The text entered by the user (set by get_user_input node)
    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)
    - llm_response: The response generated by the selected LLM (set by call_llama or call_qwen)
    - verbose: Boolean flag controlling tracing output (set by get_user_input node)
    - reprompt: Boolean flag indicating whether to re-prompt for input
    - use_qwen: Boolean flag indicating which model to use (set by get_user_input node)

    State Flow:
    1. Initial state: all fields empty/default
    2. After get_user_input: user_input, should_exit, verbose, reprompt, and use_qwen are populated
    3. After route_to_model: routes to either call_llama or call_qwen based on use_qwen flag
    4. After call_llama OR call_qwen: llm_response is populated
    5. After print_response: state unchanged (node only reads, doesn't write)

    The graph has conditional routing:
        get_user_input -> [conditional] -> call_llama -> print_response -> get_user_input
                |                      |-> call_qwen  -> print_response -> get_user_input
                |
                +-> END (if user wants to quit)
                +-> get_user_input (reprompt/mode toggle)
    """
    user_input: str
    should_exit: bool
    llm_response: str
    verbose: bool
    reprompt: bool
    use_qwen: bool

def create_llm(model_id: str, model_name: str):
    """
    Create and configure an LLM using HuggingFace's transformers library.
    Downloads the specified model from HuggingFace Hub and wraps it
    for use with LangChain via HuggingFacePipeline.
    
    Args:
        model_id: The HuggingFace model identifier (e.g., "meta-llama/Llama-3.2-1B-Instruct")
        model_name: A friendly name for logging (e.g., "Llama")
    """
    # Get the optimal device for inference
    device = get_device()

    print(f"Loading {model_name} model: {model_id}")
    print("This may take a moment on first run as the model is downloaded...")

    # Load the tokenizer - converts text to tokens the model understands
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Load the model itself with appropriate settings for the device
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    # Move model to MPS device if using Apple Silicon
    if device == "mps":
        model = model.to(device)

    # Create a text generation pipeline that combines model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Maximum tokens to generate in response
        do_sample=True,      # Enable sampling for varied responses
        temperature=0.7,     # Controls randomness (lower = more deterministic)
        top_p=0.95,          # Nucleus sampling parameter
        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning
    )

    # Wrap the HuggingFace pipeline for use with LangChain
    llm = HuggingFacePipeline(pipeline=pipe)

    print(f"{model_name} model loaded successfully!")
    return llm

def create_graph(llama_llm, qwen_llm):
    """
    Create the LangGraph state graph with conditional model routing:
    1. get_user_input: Reads input from stdin and determines which model to use
    2. call_llama: Sends input to Llama LLM (conditional)
    3. call_qwen: Sends input to Qwen LLM (conditional)
    4. print_response: Prints the LLM response to stdout

    Graph structure with conditional routing:
        START -> get_user_input -> [conditional] -> call_llama -> print_response -+
                       ^                 |                                         |
                       |                 +-> call_qwen -> print_response ----------+
                       |                 |                                         |
                       |                 +-> END (quit)                            |
                       |                 +-> get_user_input (reprompt/mode toggle)|
                       |                                                           |
                       +-----------------------------------------------------------+

    The graph runs continuously until the user types 'quit', 'exit', or 'q'.
    Special commands:
    - 'verbose': Enable tracing output from all nodes
    - 'quiet': Disable tracing output from all nodes
    
    Model selection:
    - Input starting with "Hey Qwen" -> routes to Qwen model
    - All other input -> routes to Llama model
    """

    # =========================================================================
    # NODE 1: get_user_input
    # =========================================================================
    def get_user_input(state: AgentState) -> dict:
        """
        Node that prompts the user for input via stdin and determines which model to use.

        Reads state: 
            - verbose: Current verbose mode setting
        Updates state:
            - user_input: The raw text entered by the user
            - should_exit: True if user wants to quit, False otherwise
            - verbose: Updated if user entered "verbose" or "quiet"
            - reprompt: True if empty input or mode toggle
            - use_qwen: True if input starts with "Hey Qwen", False otherwise
        """
        # Get current verbose setting from state
        verbose = state.get("verbose", False)
        
        # Print tracing information if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: get_user_input")
            print(f"[TRACE] Current state - verbose: {verbose}")
        
        # Display banner before each prompt
        print("\n" + "=" * 50)
        print("Enter your text (or 'quit' to exit, 'verbose'/'quiet' to toggle tracing):")
        print("Tip: Start with 'Hey Qwen' to use Qwen model, otherwise Llama will be used")
        print("=" * 50)

        print("\n> ", end="")
        user_input = input()

        # Check for special commands
        if user_input.lower() in ['quit', 'exit', 'q']:
            if verbose:
                print("[TRACE] User requested exit")
            print("Goodbye!")
            return {
                "user_input": user_input,
                "should_exit": True,
                "verbose": verbose,
                "reprompt": False,
                "use_qwen": state.get("use_qwen", False),
                "llm_response": state.get("llm_response", ""),

            }
        
        # Handle verbose mode toggle
        if user_input.lower() == 'verbose':
            print("Verbose mode enabled - tracing information will be displayed")
            if verbose:
                print("[TRACE] Verbose mode was already enabled")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": True,
                "reprompt": True,
                "use_qwen": state.get("use_qwen", False),
                "llm_response": state.get("llm_response", ""),

            }
        
        # Handle quiet mode toggle
        if user_input.lower() == 'quiet':
            if verbose:
                print("[TRACE] Disabling verbose mode")
            print("Quiet mode enabled - tracing information will be hidden")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": False,
                "reprompt": True,
                "use_qwen": state.get("use_qwen", False),
                "llm_response": state.get("llm_response", ""),

            }

        # If the user submitted an empty line, don't call the LLMs — reprompt.
        if user_input.strip() == "":
            if verbose:
                print("[TRACE] Empty input received — reprompting for input")
            return {
                "user_input": "",
                "should_exit": False,
                "reprompt": True,
                "verbose": verbose, 
                "use_qwen": state.get("use_qwen", False),
                "llm_response": state.get("llm_response", ""),

            }
        
        # Determine which model to use based on input
        use_qwen = user_input.strip().lower().startswith("hey qwen")
        
        # Valid input - determine which model to route to
        if verbose:
            print(f"[TRACE] Valid user input received: '{user_input}'")
            print(f"[TRACE] Model selection: {'Qwen' if use_qwen else 'Llama'}")
            print("[TRACE] Exiting node: get_user_input")
        else:
            # Inform user which model will be used
            print(f"\nUsing: {'Qwen' if use_qwen else 'Llama'} model")
        
        return {
            "user_input": user_input,
            "should_exit": False,
            "verbose": verbose,
            "reprompt": False,
            "use_qwen": use_qwen,
        }

    # =========================================================================
    # NODE 2: call_llama
    # =========================================================================
    def call_llama(state: AgentState) -> dict:
        """
        Node that invokes the Llama LLM with the user's input.

        Reads state:
            - user_input: The text to send to the LLM
            - verbose: Whether to print tracing information
        Updates state:
            - llm_response: The text generated by Llama
        """
        user_input = state["user_input"]
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Entering node: call_llama")
            print(f"[TRACE] Input to Llama: '{user_input}'")

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted Llama prompt: '{prompt}'")
        
        print("\nProcessing with Llama...")

        # Invoke the LLM and get the response
        response = llama_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Llama response received (length: {len(response)} chars)")
            print("[TRACE] Exiting node: call_llama")

        return {"llm_response": response}

    # =========================================================================
    # NODE 3: call_qwen
    # =========================================================================
    def call_qwen(state: AgentState) -> dict:
        """
        Node that invokes the Qwen LLM with the user's input.

        Reads state:
            - user_input: The text to send to the LLM
            - verbose: Whether to print tracing information
        Updates state:
            - llm_response: The text generated by Qwen
        """
        user_input = state["user_input"]
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Entering node: call_qwen")
            print(f"[TRACE] Input to Qwen: '{user_input}'")

        # Format the prompt for the instruction-tuned model
        # Strip "Hey Qwen" prefix if present for cleaner prompt
        clean_input = user_input
        if user_input.strip().lower().startswith("hey qwen"):
            raw = user_input.strip()
            lower = raw.lower()
            if lower.startswith("hey qwen"):
              clean_input = raw[len("hey qwen"):].lstrip(" ,:-")
            else:
              clean_input = raw

            if verbose:
                print(f"[TRACE] Stripped 'Hey Qwen' prefix, clean input: '{clean_input}'")
        
        prompt = f"User: {clean_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted Qwen prompt: '{prompt}'")
        
        print("\nProcessing with Qwen...")

        # Invoke the LLM and get the response
        response = qwen_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Qwen response received (length: {len(response)} chars)")
            print("[TRACE] Exiting node: call_qwen")

        return {"llm_response": response}

    # =========================================================================
    # NODE 4: print_response
    # =========================================================================
    def print_response(state: AgentState) -> dict:
        """
        Node that prints the LLM response to stdout.

        Reads state:
            - llm_response: The text to print
            - verbose: Whether to print tracing information
            - use_qwen: Which model was used (for display purposes)
        Updates state:
            - Nothing (returns empty dict, state unchanged)
        """
        verbose = state.get("verbose", False)
        use_qwen = state.get("use_qwen", False)
        
        if verbose:
            print("\n[TRACE] Entering node: print_response")
            print(f"[TRACE] Response length: {len(state['llm_response'])} characters")
            print(f"[TRACE] Model used: {'Qwen' if use_qwen else 'Llama'}")
        
        print("\n" + "=" * 70)
        print(f"{'QWEN 2.5 0.5B' if use_qwen else 'LLAMA 3.2 1B'} RESPONSE:")
        print("=" * 70)
        print(state["llm_response"])

        if verbose:
            print("\n[TRACE] Response printed to stdout")
            print("[TRACE] Exiting node: print_response")
            print("[TRACE] Looping back to get_user_input")

        return {}

    # =========================================================================
    # ROUTING FUNCTION
    # =========================================================================
    def route_after_input(state: AgentState) -> str:
        """
        Routing function that determines the next node based on state.

        Examines state:
            - should_exit: If True, terminate the graph
            - reprompt: If True, loop back to get_user_input
            - use_qwen: If True, route to call_qwen, otherwise call_llama
            - verbose: For tracing output

        Returns:
            - "__end__": If user wants to quit
            - "get_user_input": If reprompt (empty input or mode toggle)
            - "call_qwen": If use_qwen flag is True
            - "call_llama": If use_qwen flag is False (default)
        """
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Routing decision after get_user_input")
        
        # Check if user wants to exit
        if state.get("should_exit", False):
            if verbose:
                print("[TRACE] Routing to: END")
            return END

        # If get_user_input indicated reprompt (empty input or mode toggle), route back
        if state.get("reprompt", False):
            if verbose:
                print("[TRACE] Reprompt flag set, routing to: get_user_input")
            return "get_user_input"

        # Route to appropriate model based on use_qwen flag
        if state.get("use_qwen", False):
            if verbose:
                print("[TRACE] use_qwen=True, routing to: call_qwen")
            return "call_qwen"
        else:
            if verbose:
                print("[TRACE] use_qwen=False, routing to: call_llama")
            return "call_llama"

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================
    # Create a StateGraph with our defined state structure
    graph_builder = StateGraph(AgentState)

    # Add all nodes to the graph
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("call_llama", call_llama)
    graph_builder.add_node("call_qwen", call_qwen)
    graph_builder.add_node("print_response", print_response)

    # Define edges:
    # 1. START -> get_user_input (always start by getting user input)
    graph_builder.add_edge(START, "get_user_input")

    # 2. get_user_input -> [conditional] -> call_llama OR call_qwen OR get_user_input OR END
    graph_builder.add_conditional_edges(
        "get_user_input",
        route_after_input,
        {
            "call_llama": "call_llama",          # Default: use Llama
            "call_qwen": "call_qwen",            # "Hey Qwen" prefix: use Qwen
            "get_user_input": "get_user_input",  # Reprompt -> loop back
            END: END                              # Quit -> terminate
        }
    )

    # 3. call_llama -> print_response
    graph_builder.add_edge("call_llama", "print_response")
    
    # 4. call_qwen -> print_response
    graph_builder.add_edge("call_qwen", "print_response")

    # 5. print_response -> get_user_input (loop back for next input)
    graph_builder.add_edge("print_response", "get_user_input")

    # Compile the graph into an executable form
    graph = graph_builder.compile()

    return graph

def save_graph_image(graph, filename="lg_graph.png"):
    """
    Generate a Mermaid diagram of the graph and save it as a PNG image.
    Uses the graph's built-in Mermaid export functionality.
    """
    try:
        # Get the Mermaid PNG representation of the graph
        # This requires the 'grandalf' package for rendering
        png_data = graph.get_graph(xray=True).draw_mermaid_png()

        # Write the PNG data to file
        with open(filename, "wb") as f:
            f.write(png_data)

        print(f"Graph image saved to {filename}")
    except Exception as e:
        print(f"Could not save graph image: {e}")
        print("You may need to install additional dependencies: pip install grandalf")

def main():
    """
    Main function that orchestrates the conditional model routing workflow:
    1. Initialize both LLMs (Llama and Qwen)
    2. Create the LangGraph with conditional routing
    3. Save the graph visualization
    4. Run the graph (it loops internally until user quits)

    The graph handles conditional routing:
    - get_user_input: Prompts, reads from stdin, and sets use_qwen flag
    - Router: Directs to call_llama or call_qwen based on use_qwen flag
    - call_llama OR call_qwen: Process input with selected model
    - print_response: Prints result
    - Loop back to get_user_input

    The graph only terminates when the user types 'quit', 'exit', or 'q'.
    
    Special commands:
    - Type "verbose" to enable tracing information in all nodes
    - Type "quiet" to disable tracing information
    - Start input with "Hey Qwen" to route to Qwen model
    - Otherwise, Llama model will be used
    """
    print("=" * 70)
    print("LangGraph Conditional Models: Llama 3.2 1B + Qwen 2.5 0.5B")
    print("=" * 70)
    print()

    # Step 1: Create and configure both LLMs
    print("Loading Llama model...")
    llama_llm = create_llm("meta-llama/Llama-3.2-1B-Instruct", "Llama")
    
    print("\nLoading Qwen model...")
    qwen_llm = create_llm("Qwen/Qwen2.5-0.5B-Instruct", "Qwen")

    # Step 2: Build the LangGraph with conditional routing
    print("\nCreating LangGraph with conditional model routing...")
    graph = create_graph(llama_llm, qwen_llm)
    print("Graph created successfully!")

    # Step 3: Save a visual representation of the graph before execution
    print("\nSaving graph visualization...")
    save_graph_image(graph)

    # Step 4: Run the graph - it will loop internally until user quits
    initial_state: AgentState = {
        "user_input": "",
        "should_exit": False,
        "llm_response": "",
        "verbose": False,
        "reprompt": False,
        "use_qwen": False,
    }

    # Single invocation - the graph loops internally
    # Model selection is based on input prefix
    graph.invoke(initial_state)

# Entry point - only run main() if this script is executed directly
if __name__ == "__main__":
    main()