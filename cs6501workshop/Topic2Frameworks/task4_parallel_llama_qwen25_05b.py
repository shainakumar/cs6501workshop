# langgraph_parallel_models.py
# Program demonstrates use of LangGraph with parallel model execution.
# It writes to stdout and asks the user to enter a line of text through stdin.
# It passes the line to TWO LLMs in parallel: llama-3.2-1B-Instruct and Qwen2.5-0.5B-Instruct,
# then prints what both LLMs return as text to stdout.
# The LLMs should use Cuda if available, if not then if mps is available then use that,
# otherwise use cpu.
# After the LangGraph graph is created but before it executes, the program
# uses the Mermaid library to write a image of the graph to the file lg_graph.png
# The program gets the LLMs from Hugging Face and wraps them for LangChain using HuggingFacePipeline.
# The code is commented in detail so a reader can understand each step.
# Supports verbose mode: type "verbose" to enable tracing, "quiet" to disable.

# Import necessary libraries
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain_huggingface import HuggingFacePipeline
from langgraph.graph import StateGraph, START, END
from typing import TypedDict

# Determine the best available device for inference
# Priority: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
def get_device():
    """
    Detect and return the best available compute device.
    Returns 'cuda' for NVIDIA GPUs, 'mps' for Apple Silicon, or 'cpu' as fallback.
    """
    if torch.cuda.is_available():
        print("Using CUDA (NVIDIA GPU) for inference")
        return "cuda"
    elif torch.backends.mps.is_available():
        print("Using MPS (Apple Silicon) for inference")
        return "mps"
    else:
        print("Using CPU for inference")
        return "cpu"

# =============================================================================
# STATE DEFINITION
# =============================================================================
# The state is a TypedDict that flows through all nodes in the graph.
# Each node can read from and write to specific fields in the state.
# LangGraph automatically merges the returned dict from each node into the state.

class AgentState(TypedDict):
    """
    State object that flows through the LangGraph nodes.

    Fields:
    - user_input: The text entered by the user (set by get_user_input node)
    - should_exit: Boolean flag indicating if user wants to quit (set by get_user_input node)
    - llama_response: The response generated by Llama (set by call_llama node)
    - qwen_response: The response generated by Qwen (set by call_qwen node)
    - verbose: Boolean flag controlling tracing output (set by get_user_input node)
    - reprompt: Boolean flag indicating whether to re-prompt for input

    State Flow:
    1. Initial state: all fields empty/default
    2. After get_user_input: user_input, should_exit, verbose, and reprompt are populated
    3. After prepare_input: state unchanged (just passes through)
    4. After call_llama (parallel): llama_response is populated
    5. After call_qwen (parallel): qwen_response is populated
    6. After print_responses: state unchanged (node only reads, doesn't write)

    The graph has parallel execution:
        get_user_input -> [conditional] -> prepare_input -> [parallel] -> call_llama
                |                                                      |-> call_qwen
                |                                                              |
                +-> END (if user wants to quit)                                v
                                                                        print_responses
                                                                               |
                                                                               v
                                                                        get_user_input
    """
    user_input: str
    should_exit: bool
    llama_response: str
    qwen_response: str
    verbose: bool
    reprompt: bool
    printed: bool


def create_llm(model_id: str, model_name: str):
    """
    Create and configure an LLM using HuggingFace's transformers library.
    Downloads the specified model from HuggingFace Hub and wraps it
    for use with LangChain via HuggingFacePipeline.
    
    Args:
        model_id: The HuggingFace model identifier (e.g., "meta-llama/Llama-3.2-1B-Instruct")
        model_name: A friendly name for logging (e.g., "Llama")
    """
    # Get the optimal device for inference
    device = get_device()

    print(f"Loading {model_name} model: {model_id}")
    print("This may take a moment on first run as the model is downloaded...")

    # Load the tokenizer - converts text to tokens the model understands
    tokenizer = AutoTokenizer.from_pretrained(model_id)

    # Load the model itself with appropriate settings for the device
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.float16 if device != "cpu" else torch.float32,
        device_map=device if device == "cuda" else None,
    )

    # Move model to MPS device if using Apple Silicon
    if device == "mps":
        model = model.to(device)

    # Create a text generation pipeline that combines model and tokenizer
    pipe = pipeline(
        "text-generation",
        model=model,
        tokenizer=tokenizer,
        max_new_tokens=256,  # Maximum tokens to generate in response
        do_sample=True,      # Enable sampling for varied responses
        temperature=0.7,     # Controls randomness (lower = more deterministic)
        top_p=0.95,          # Nucleus sampling parameter
        pad_token_id=tokenizer.eos_token_id,  # Suppress pad_token_id warning
    )

    # Wrap the HuggingFace pipeline for use with LangChain
    llm = HuggingFacePipeline(pipeline=pipe)

    print(f"{model_name} model loaded successfully!")
    return llm

def create_graph(llama_llm, qwen_llm):
    """
    Create the LangGraph state graph with nodes for parallel model execution:
    1. get_user_input: Reads input from stdin
    2. prepare_input: Pass-through node that routes to parallel nodes
    3. call_llama: Sends input to Llama LLM (runs in parallel)
    4. call_qwen: Sends input to Qwen LLM (runs in parallel)
    5. print_responses: Prints both LLM responses to stdout

    Graph structure with parallel execution:
        START -> get_user_input -> [conditional] -> prepare_input -> call_llama -> print_responses
                       ^                 |                      |-> call_qwen  ->        |
                       |                 |                                               |
                       |                 +-> END (quit)                                  |
                       |                 +-> get_user_input (reprompt/mode toggle)       |
                       |                                                                 |
                       +-----------------------------------------------------------------+

    The graph runs continuously until the user types 'quit', 'exit', or 'q'.
    Special commands:
    - 'verbose': Enable tracing output from all nodes
    - 'quiet': Disable tracing output from all nodes
    """

    # =========================================================================
    # NODE 1: get_user_input
    # =========================================================================
    def get_user_input(state: AgentState) -> dict:
        """
        Node that prompts the user for input via stdin.

        Reads state: 
            - verbose: Current verbose mode setting
        Updates state:
            - user_input: The raw text entered by the user
            - should_exit: True if user wants to quit, False otherwise
            - verbose: Updated if user entered "verbose" or "quiet"
            - reprompt: True if empty input or mode toggle
        """
        # Get current verbose setting from state
        verbose = state.get("verbose", False)
        
        # Print tracing information if verbose mode is enabled
        if verbose:
            print("\n[TRACE] Entering node: get_user_input")
            print(f"[TRACE] Current state - verbose: {verbose}")
        
        # Display banner before each prompt
        print("\n" + "=" * 50)
        print("Enter your text (or 'quit' to exit, 'verbose'/'quiet' to toggle tracing):")
        print("=" * 50)

        print("\n> ", end="")
        user_input = input()

        # Check for special commands
        if user_input.lower() in ['quit', 'exit', 'q']:
            if verbose:
                print("[TRACE] User requested exit")
            print("Goodbye!")
            return {
                "user_input": user_input,
                "should_exit": True,
                "verbose": verbose,
            }
        
        # Handle verbose mode toggle
        if user_input.lower() == 'verbose':
            print("Verbose mode enabled - tracing information will be displayed")
            if verbose:
                print("[TRACE] Verbose mode was already enabled")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": True,
                "reprompt": True
            }
        
        # Handle quiet mode toggle
        if user_input.lower() == 'quiet':
            if verbose:
                print("[TRACE] Disabling verbose mode")
            print("Quiet mode enabled - tracing information will be hidden")
            return {
                "user_input": user_input,
                "should_exit": False,
                "verbose": False,
                "reprompt": True
            }

        # If the user submitted an empty line, don't call the LLMs — reprompt.
        if user_input.strip() == "":
            if verbose:
                print("[TRACE] Empty input received — reprompting for input")
            return {
                "user_input": "",
                "should_exit": False,
                "reprompt": True,
                "verbose": verbose, 
            }
        
        # Valid input - continue to prepare_input
        if verbose:
            print(f"[TRACE] Valid user input received: '{user_input}'")
            print("[TRACE] Exiting node: get_user_input")
        
        return {
            "user_input": user_input,
            "should_exit": False,
            "verbose": verbose,
            "reprompt": False,
            "printed": False,
            "llama_response": "",
            "qwen_response": "",
        }

    # =========================================================================
    # NODE 2: prepare_input
    # =========================================================================
    def prepare_input(state: AgentState) -> dict:
        """
        Pass-through node that prepares input for parallel LLM calls.
        This node exists to create a clear fan-out point in the graph.

        Reads state:
            - user_input: The text to process
            - verbose: Whether to print tracing information
        Updates state:
            - Nothing (pass-through node)
        """
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Entering node: prepare_input")
            print(f"[TRACE] Preparing to send input to both Llama and Qwen models")
            print(f"[TRACE] Input: '{state['user_input']}'")
            print("[TRACE] Exiting node: prepare_input")
        
        print("\nProcessing your input with both models in parallel...")
        
        # Return empty dict - this is a pass-through node
        return {}

    # =========================================================================
    # NODE 3: call_llama
    # =========================================================================
    def call_llama(state: AgentState) -> dict:
        """
        Node that invokes the Llama LLM with the user's input.
        Runs in parallel with call_qwen.

        Reads state:
            - user_input: The text to send to the LLM
            - verbose: Whether to print tracing information
        Updates state:
            - llama_response: The text generated by Llama
        """
        user_input = state["user_input"]
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Entering node: call_llama")
            print(f"[TRACE] Input to Llama: '{user_input}'")

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted Llama prompt: '{prompt}'")
        
        # Invoke the LLM and get the response
        response = llama_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Llama response received (length: {len(response)} chars)")
            print("[TRACE] Exiting node: call_llama")

        return {"llama_response": response}

    # =========================================================================
    # NODE 4: call_qwen
    # =========================================================================
    def call_qwen(state: AgentState) -> dict:
        """
        Node that invokes the Qwen LLM with the user's input.
        Runs in parallel with call_llama.

        Reads state:
            - user_input: The text to send to the LLM
            - verbose: Whether to print tracing information
        Updates state:
            - qwen_response: The text generated by Qwen
        """
        user_input = state["user_input"]
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Entering node: call_qwen")
            print(f"[TRACE] Input to Qwen: '{user_input}'")

        # Format the prompt for the instruction-tuned model
        prompt = f"User: {user_input}\nAssistant:"

        if verbose:
            print(f"[TRACE] Formatted Qwen prompt: '{prompt}'")
        
        # Invoke the LLM and get the response
        response = qwen_llm.invoke(prompt)

        if verbose:
            print(f"[TRACE] Qwen response received (length: {len(response)} chars)")
            print("[TRACE] Exiting node: call_qwen")

        return {"qwen_response": response}

    # =========================================================================
    # NODE 5: print_responses
    # =========================================================================
    def print_responses(state: AgentState) -> dict:
      verbose = state.get("verbose", False)

      # don't print twice
      if state.get("printed", False):
          return {}

      llama = state.get("llama_response", "")
      qwen = state.get("qwen_response", "")

      # wait until both are ready
      if not llama or not qwen:
          if verbose:
              print("[TRACE] print_responses waiting for both model outputs...")
          return {}

      if verbose:
          print("\n[TRACE] Entering node: print_responses")
          print(f"[TRACE] Llama response length: {len(llama)} characters")
          print(f"[TRACE] Qwen response length: {len(qwen)} characters")

      print("\n" + "=" * 70)
      print("LLAMA 3.2 1B RESPONSE:")
      print("=" * 70)
      print(llama)

      print("\n" + "=" * 70)
      print("QWEN 2.5 0.5B RESPONSE:")
      print("=" * 70)
      print(qwen)

      if verbose:
          print("\n[TRACE] Both responses printed to stdout")
          print("[TRACE] Exiting node: print_responses")

      # Mark as printed so if this node fires again, it won't print twice
      return {"printed": True}


    # =========================================================================
    # ROUTING FUNCTION
    # =========================================================================
    def route_after_input(state: AgentState) -> str:
        """
        Routing function that determines the next node based on state.

        Examines state:
            - should_exit: If True, terminate the graph
            - reprompt: If True, loop back to get_user_input
            - verbose: For tracing output

        Returns:
            - "__end__": If user wants to quit
            - "get_user_input": If reprompt (empty input or mode toggle)
            - "prepare_input": If valid input to process
        """
        verbose = state.get("verbose", False)
        
        if verbose:
            print("\n[TRACE] Routing decision after get_user_input")
        
        # Check if user wants to exit
        if state.get("should_exit", False):
            if verbose:
                print("[TRACE] Routing to: END")
            return END

        # If get_user_input indicated reprompt (empty input or mode toggle), route back
        if state.get("reprompt", False):
            if verbose:
                print("[TRACE] Reprompt flag set, routing to: get_user_input")
            return "get_user_input"

        # Valid input: Proceed to prepare_input, which fans out to both models
        if verbose:
            print("[TRACE] Valid input, routing to: prepare_input")
        return "prepare_input"

    # =========================================================================
    # GRAPH CONSTRUCTION
    # =========================================================================
    # Create a StateGraph with our defined state structure
    graph_builder = StateGraph(AgentState)

    # Add all nodes to the graph
    graph_builder.add_node("get_user_input", get_user_input)
    graph_builder.add_node("prepare_input", prepare_input)
    graph_builder.add_node("call_llama", call_llama)
    graph_builder.add_node("call_qwen", call_qwen)
    graph_builder.add_node("print_responses", print_responses)

    # Define edges:
    # 1. START -> get_user_input (always start by getting user input)
    graph_builder.add_edge(START, "get_user_input")

    # 2. get_user_input -> [conditional] -> prepare_input OR get_user_input OR END
    graph_builder.add_conditional_edges(
        "get_user_input",
        route_after_input,
        {
            "prepare_input": "prepare_input",    # Valid input -> prepare for parallel execution
            "get_user_input": "get_user_input",  # Reprompt -> loop back
            END: END                              # Quit -> terminate
        }
    )

    # 3. prepare_input -> call_llama (parallel execution starts here)
    graph_builder.add_edge("prepare_input", "call_llama")
    
    # 4. prepare_input -> call_qwen (parallel execution)
    graph_builder.add_edge("prepare_input", "call_qwen")

    # 5. Both call_llama and call_qwen -> print_responses
    #    LangGraph automatically waits for both parallel nodes to complete
    graph_builder.add_edge("call_llama", "print_responses")
    graph_builder.add_edge("call_qwen", "print_responses")

    # 6. print_responses -> get_user_input (loop back for next input)
    graph_builder.add_edge("print_responses", "get_user_input")

    # Compile the graph into an executable form
    graph = graph_builder.compile()

    return graph

def save_graph_image(graph, filename="lg_graph.png"):
    """
    Generate a Mermaid diagram of the graph and save it as a PNG image.
    Uses the graph's built-in Mermaid export functionality.
    """
    try:
        # Get the Mermaid PNG representation of the graph
        # This requires the 'grandalf' package for rendering
        png_data = graph.get_graph(xray=True).draw_mermaid_png()

        # Write the PNG data to file
        with open(filename, "wb") as f:
            f.write(png_data)

        print(f"Graph image saved to {filename}")
    except Exception as e:
        print(f"Could not save graph image: {e}")
        print("You may need to install additional dependencies: pip install grandalf")

def main():
    """
    Main function that orchestrates the parallel model agent workflow:
    1. Initialize both LLMs (Llama and Qwen)
    2. Create the LangGraph with parallel execution
    3. Save the graph visualization
    4. Run the graph (it loops internally until user quits)

    The graph handles parallel execution:
    - get_user_input: Prompts and reads from stdin
    - prepare_input: Pass-through node that fans out to parallel nodes
    - call_llama & call_qwen: Process input in parallel
    - print_responses: Waits for both, then prints results
    - Loop back to get_user_input

    The graph only terminates when the user types 'quit', 'exit', or 'q'.
    
    Special commands:
    - Type "verbose" to enable tracing information in all nodes
    - Type "quiet" to disable tracing information
    """
    print("=" * 70)
    print("LangGraph Parallel Models: Llama 3.2 1B + Qwen 2.5 0.5B")
    print("=" * 70)
    print()

    # Step 1: Create and configure both LLMs
    print("Loading Llama model...")
    llama_llm = create_llm("meta-llama/Llama-3.2-1B-Instruct", "Llama")
    
    print("\nLoading Qwen model...")
    qwen_llm = create_llm("Qwen/Qwen2.5-0.5B-Instruct", "Qwen")

    # Step 2: Build the LangGraph with both LLMs
    print("\nCreating LangGraph with parallel execution...")
    graph = create_graph(llama_llm, qwen_llm)
    print("Graph created successfully!")

    # Step 3: Save a visual representation of the graph before execution
    print("\nSaving graph visualization...")
    save_graph_image(graph)

    # Step 4: Run the graph - it will loop internally until user quits
    initial_state: AgentState = {
        "user_input": "",
        "should_exit": False,
        "llama_response": "",
        "qwen_response": "",
        "verbose": False,
        "reprompt": False,
        "printed": False,
    }

    # Single invocation - the graph loops internally
    # Both models run in parallel for each user input
    graph.invoke(initial_state)

# Entry point - only run main() if this script is executed directly
if __name__ == "__main__":
    main()